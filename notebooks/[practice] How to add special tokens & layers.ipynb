{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2adf768-0356-404a-b445-890928010154",
   "metadata": {},
   "source": [
    "### 1. Entity에 따른 Special Token을 문장에 추가해보자!\n",
    "### 2. Entity 유무를 나타내는 embedding layer를 추가해보자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ad91906-80c2-49c0-a08b-ad1b3bd0c13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla V100-PCIE-32GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tarfile\n",
    "import pickle as pickle\n",
    "from tqdm import tqdm\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Using KoELECTRA Model\n",
    "from transformers import ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\n",
    "\n",
    "# Added by Me\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from ohsuz.utils import *\n",
    "from ohsuz.loss import *\n",
    "from ohsuz.config import *\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a2cdf7-807b-4f1c-8f59-dfab5c3c4be9",
   "metadata": {},
   "source": [
    "### **1. Entity에 따른 Special Token을 문장에 추가해보자!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95d5bc91-0252-4181-bde8-c8a7c96e88de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(os.path.join(train_dir, 'train.tsv'), delimiter='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96bd318c-7cc1-4405-9ad7-883f86f30f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wikipedia-24896-25-30-33-19-21</td>\n",
       "      <td>영국에서 사용되는 스포츠 유틸리티 자동차의 브랜드로는 랜드로버(Land Rover)...</td>\n",
       "      <td>랜드로버</td>\n",
       "      <td>30</td>\n",
       "      <td>33</td>\n",
       "      <td>자동차</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>단체:제작</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wikipedia-12728-224-5-7-42-44</td>\n",
       "      <td>선거에서 민주당은 해산 전 의석인 230석에 한참 못 미치는 57석(지역구 27석,...</td>\n",
       "      <td>민주당</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>27석</td>\n",
       "      <td>42</td>\n",
       "      <td>44</td>\n",
       "      <td>관계_없음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wikipedia-28460-3-0-7-9-12</td>\n",
       "      <td>유럽 축구 연맹(UEFA) 집행위원회는 2014년 1월 24일에 열린 회의를 통해 ...</td>\n",
       "      <td>유럽 축구 연맹</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>UEFA</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>단체:별칭</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wikipedia-11479-37-24-26-3-5</td>\n",
       "      <td>용병 공격수 챠디의 부진과 시즌 초 활약한 강수일의 침체, 시즌 중반에 영입한 세르...</td>\n",
       "      <td>강수일</td>\n",
       "      <td>24</td>\n",
       "      <td>26</td>\n",
       "      <td>공격수</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>인물:직업/직함</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wikipedia-15581-6-0-2-32-40</td>\n",
       "      <td>람캄행 왕은 1237년에서 1247년 사이 수코타이의 왕 퍼쿤 씨 인트라팃과 쓰엉 ...</td>\n",
       "      <td>람캄행</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>퍼쿤 씨 인트라팃</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>인물:부모님</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                0  \\\n",
       "0  wikipedia-24896-25-30-33-19-21   \n",
       "1   wikipedia-12728-224-5-7-42-44   \n",
       "2      wikipedia-28460-3-0-7-9-12   \n",
       "3    wikipedia-11479-37-24-26-3-5   \n",
       "4     wikipedia-15581-6-0-2-32-40   \n",
       "\n",
       "                                                   1         2   3   4  \\\n",
       "0  영국에서 사용되는 스포츠 유틸리티 자동차의 브랜드로는 랜드로버(Land Rover)...      랜드로버  30  33   \n",
       "1  선거에서 민주당은 해산 전 의석인 230석에 한참 못 미치는 57석(지역구 27석,...       민주당   5   7   \n",
       "2  유럽 축구 연맹(UEFA) 집행위원회는 2014년 1월 24일에 열린 회의를 통해 ...  유럽 축구 연맹   0   7   \n",
       "3  용병 공격수 챠디의 부진과 시즌 초 활약한 강수일의 침체, 시즌 중반에 영입한 세르...       강수일  24  26   \n",
       "4  람캄행 왕은 1237년에서 1247년 사이 수코타이의 왕 퍼쿤 씨 인트라팃과 쓰엉 ...       람캄행   0   2   \n",
       "\n",
       "           5   6   7         8  \n",
       "0        자동차  19  21     단체:제작  \n",
       "1        27석  42  44     관계_없음  \n",
       "2       UEFA   9  12     단체:별칭  \n",
       "3        공격수   3   5  인물:직업/직함  \n",
       "4  퍼쿤 씨 인트라팃  32  40    인물:부모님  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "781644ac-b538-4e2d-bba2-7f97e412e97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "practice = dataset[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9a811b4-35ae-40b2-ba18-7024714530cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1, a2, b1, b2 = dataset[3][0], dataset[4][0], dataset[6][0], dataset[7][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8266456-f41b-461f-a951-f5d55b70962b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영국에서 사용되는 스포츠 유틸리티 <E02>자동차</E02>의 브랜드로는 <E01>랜드로버</E01>(Land Rover)와 지프(Jeep)가 있으며, 이 브랜드들은 자동차의 종류를 일컫는 말로 사용되기도 한다.\n"
     ]
    }
   ],
   "source": [
    "if a1 > b1: # b1 먼저\n",
    "    new = practice[:b1] + \"<E02>\" + practice[b1:b2+1] + \"</E02>\" + practice[b2+1:a1] + \"<E01>\" + practice[a1:a2+1] + \"</E01>\" + practice[a2+1:]\n",
    "    print(new)\n",
    "else: # a1 먼저\n",
    "    new = practice[:a1] + \"<E01>\" + practice[a1:a2+1] + \"</E01>\" + practice[a2+1:b1] + \"<E02>\" + practice[b1:b2+1] + \"</E02>\" + practice[b2+1:]\n",
    "    print(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c67a27d6-5e18-4801-86ac-1a30b71c475b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원하는 entity 네임을 지정할 수도 있게 하고 싶은데 그러면 파라미터가 너무 많아진다\n",
    "def add_entity_tokens(sentence, a1, a2, b1, b2):\n",
    "    new_sentence = None\n",
    "    if a1 > b1: # b1 먼저\n",
    "        new_sentence = sentence[:b1] + \"[E02]\" + sentence[b1:b2+1] + \"[/E02]\" + sentence[b2+1:a1] + \"[E01]\" + sentence[a1:a2+1] + \"[/E01]\" + sentence[a2+1:]\n",
    "    else: # a1 먼저\n",
    "        new_sentence = sentence[:a1] + \"[E01]\" + sentence[a1:a2+1] + \"[/E01]\" + sentence[a2+1:b1] + \"[E02]\" + sentence[b1:b2+1] + \"[/E02]\" + sentence[b2+1:]\n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d916acb-dec8-49e7-a83b-9a479aa65a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영국에서 사용되는 스포츠 유틸리티 [E02]자동차[/E02]의 브랜드로는 [E01]랜드로버[/E01](Land Rover)와 지프(Jeep)가 있으며, 이 브랜드들은 자동차의 종류를 일컫는 말로 사용되기도 한다.\n"
     ]
    }
   ],
   "source": [
    "text= add_entity_tokens(dataset[1][0], dataset[3][0], dataset[4][0], dataset[6][0], dataset[7][0])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9f41ff-cfa5-42b5-b97f-ccc86993cf57",
   "metadata": {},
   "source": [
    "### ***※※※ 새로운 토큰을 추가해줬을 때 유의할 점 ※※※***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0cecf5-81a8-4150-8aca-4227c7bce18a",
   "metadata": {},
   "source": [
    "> pretrained model은 기존 vocab size에 맞춰있기 때문에, token을 새로 추가하는 경우 vocab size를 조정해줘야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbaeeff-0e56-4286-b5fb-e405216d8018",
   "metadata": {},
   "source": [
    "- tokenizer.add_special_tokens({'token_name':[name list]})\n",
    "- model.resize_token_embeddings(tokenizer.vocab_size + added_token_num = len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d15f5e48-896f-4890-a29f-429baa1724a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c6a636a-02ba-4d1e-a93e-41d0b1c55847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영국에서 사용되는 스포츠 유틸리티 [E02]자동차[/E02]의 브랜드로는 [E01]랜드로버[/E01](Land Rover)와 지프(Jeep)가 있으며, 이 브랜드들은 자동차의 종류를 일컫는 말로 사용되기도 한다.\n",
      "\n",
      "['영국', '##에', '##서', '사용', '##되', '##는', '스포츠', '유틸리티', '[', 'E', '##0', '##2', ']', '자동차', '[', '/', 'E', '##0', '##2', ']', '의', '브랜드', '##로', '##는', '[', 'E', '##0', '##1', ']', '랜드', '##로', '##버', '[', '/', 'E', '##0', '##1', ']', '(', 'La', '##nd', 'R', '##over', ')', '와', '지프', '(', 'Je', '##ep', ')', '가', '있', '##으며', ',', '이', '브랜드', '##들', '##은', '자동차', '##의', '종류', '##를', '일컫', '##는', '말', '##로', '사용', '##되', '##기', '##도', '한다', '.']\n"
     ]
    }
   ],
   "source": [
    "print(text, end='\\n\\n')\n",
    "print(tokenizer.tokenize(text)) # special token이 일반 token처럼 분리된 것을 확인할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e6d0c1a-4731-4b2f-b6ee-5890b2bf65fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35000\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1825ab82-efa4-4182-a4a3-a71a8e88c950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens_dict = {'additional_special_tokens': ['[E01]','[/E01]','[E02]','[/E02]']}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd66037a-60c0-4ece-88af-cf96245c20d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35004\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcbaaf5a-8571-4db6-a777-49544e3e1c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['영국', '##에', '##서', '사용', '##되', '##는', '스포츠', '유틸리티', '[E02]', '자동차', '[/E02]', '의', '브랜드', '##로', '##는', '[E01]', '랜드', '##로', '##버', '[/E01]', '(', 'La', '##nd', 'R', '##over', ')', '와', '지프', '(', 'Je', '##ep', ')', '가', '있', '##으며', ',', '이', '브랜드', '##들', '##은', '자동차', '##의', '종류', '##를', '일컫', '##는', '말', '##로', '사용', '##되', '##기', '##도', '한다', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8bca80f-13f2-458c-ae3a-f01fb04375bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'electra.embeddings.position_ids']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = ElectraForSequenceClassification.from_pretrained(\"monologg/koelectra-base-v3-discriminator\", num_labels=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec30d792-d2da-43e3-b710-48ef3596361a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(35000, 768, padding_idx=0)\n"
     ]
    }
   ],
   "source": [
    "print(model.get_input_embeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adaa7d1d-9792-468a-8d0a-2b72cfd786df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(35004, 768)\n"
     ]
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(model.get_input_embeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5c5855-9f72-4d2b-bda2-54ca907a4161",
   "metadata": {},
   "source": [
    "### **2. Entity 유무를 나타내는 embedding layer를 추가해보자!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9cb90ad0-5def-4300-982c-26cc2311283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "            text,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            max_length=190,\n",
    "            pad_to_max_length=True,\n",
    "            add_special_tokens=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "63dcfad5-44e9-45e7-87f7-365a9325a02f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,  6642,  4073,  4129,  6267,  4479,  4034,  7347, 23927, 35002,\n",
       "          6729, 35003,  3238,  6955,  4239,  4034, 35000, 10086,  4239,  4505,\n",
       "         35001,    12, 22207,  7466,    54, 29841,    13,  3170, 25982,    12,\n",
       "         25211,  9940,    13,  2010,  3249,  6460,    16,  3240,  6955,  4006,\n",
       "          4112,  6729,  4234,  7890,  4110, 16461,  4034,  2633,  4239,  6267,\n",
       "          4479,  4031,  4086,  6217,    18,     3,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "12cec198-e027-47e4-bbcd-dd5f276665af",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = inputs['input_ids'][0]\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d6a0fd56-7024-403e-bb1d-8ffbd36d932a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9138de-0a44-40af-973e-3e1b6cd39fca",
   "metadata": {},
   "source": [
    "- entity token의 id 값을 미리 구해놓고 그 사이에 있는 애들은 다 1, 아니면 다 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a0700c4a-06fa-4748-bb93-6477a4fdb053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flag = False\n",
    "special_tokens = ['[E01]','[/E01]','[E02]','[/E02]']\n",
    "is_entity_layer = []\n",
    "\n",
    "for i, token in enumerate(tokens):\n",
    "    if token in special_tokens:\n",
    "        if flag == False:\n",
    "            flag = True\n",
    "        else:\n",
    "            flag = False\n",
    "    else:\n",
    "        if flag == True:\n",
    "            is_entity_layer.append(1)\n",
    "            continue\n",
    "    is_entity_layer.append(0)\n",
    "\n",
    "is_entity_layer = torch.tensor(is_entity_layer)\n",
    "is_entity_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "452a9c50-c295-4f66-9d65-9a6b39409a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_entity_layer + inputs.attention_mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7a6fab-18d9-4669-adb1-5fedd6e1332c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
