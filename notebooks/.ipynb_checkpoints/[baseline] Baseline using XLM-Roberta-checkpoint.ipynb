{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "516c357e-2e91-4c84-aafb-0747ff5db580",
   "metadata": {},
   "source": [
    "## [0418] XLM-RoBERTa를 이용한 첫 베이스라인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ed7f8b-cdef-4dc7-9ec9-866c04786e2b",
   "metadata": {},
   "source": [
    "- **add_special_tokens 함수**: Entity 위치 정보를 활용해 [ENT],[/ENT] entity special token 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "553d190b-6eed-4c09-9d64-fc054d54e277",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mxnet in /opt/conda/lib/python3.7/site-packages (1.8.0.post0)\n",
      "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /opt/conda/lib/python3.7/site-packages (from mxnet) (1.18.5)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /opt/conda/lib/python3.7/site-packages (from mxnet) (2.23.0)\n",
      "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from mxnet) (0.8.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet) (1.25.8)\n",
      "Requirement already satisfied: gluonnlp in /opt/conda/lib/python3.7/site-packages (0.10.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (1.1.5)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (4.46.0)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from gluonnlp) (1.18.5)\n",
      "Requirement already satisfied: cython in /opt/conda/lib/python3.7/site-packages (from gluonnlp) (0.29.23)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from gluonnlp) (20.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->gluonnlp) (2.4.7)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.14.0)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (0.1.95)\n",
      "Requirement already satisfied: transformers==3 in /opt/conda/lib/python3.7/site-packages (3.0.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from transformers==3) (1.18.5)\n",
      "Requirement already satisfied: tokenizers==0.8.0-rc4 in /opt/conda/lib/python3.7/site-packages (from transformers==3) (0.8.0rc4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==3) (4.46.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==3) (2.23.0)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from transformers==3) (0.1.95)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers==3) (20.9)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==3) (2021.4.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==3) (3.0.12)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==3) (0.0.44)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3) (3.0.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==3) (2.4.7)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==3) (7.1.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==3) (1.14.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==3) (1.0.1)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (1.6.0)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch) (0.18.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch) (1.18.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install mxnet\n",
    "!pip install gluonnlp pandas tqdm\n",
    "!pip install sentencepiece\n",
    "!pip install transformers==3\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a61b8a2-2ff1-476c-bfa0-abd334cfb903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tarfile\n",
    "import pickle as pickle\n",
    "from tqdm import tqdm\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Using KoELECTRA Model\n",
    "from transformers import *\n",
    "\n",
    "# Added by Me\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from ohsuz.utils import *\n",
    "from ohsuz.loss import *\n",
    "from ohsuz.config import *\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59140c02-3c18-4028-bfdf-48de685580d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 128\n",
    "batch_size = 16\n",
    "warmup_ratio = 0.01\n",
    "epochs = 10\n",
    "max_grad_norm = 1\n",
    "log_interval = 50\n",
    "lr = 5e-5\n",
    "\n",
    "MODEL_NAME = \"xlm-roberta-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d67aaa8a-aa67-4cc6-b915-d2f79303ccb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# error labels need to be fixed later\n",
    "error_label_0 = ['wikitree-12599-4-108-111-4-7',\n",
    "                 'wikipedia-25967-115-24-26-35-37',\n",
    "                 'wikipedia-16427-6-14-17-20-22',\n",
    "                 'wikipedia-16427-8-0-3-26-28',\n",
    "                 'wikitree-19765-5-30-33-6-8',\n",
    "                 'wikitree-58702-0-18-20-22-24',\n",
    "                 'wikitree-71638-8-21-23-15-17',\n",
    "                 'wikipedia-257-0-0-1-53-57',\n",
    "                 'wikipedia-13649-28-66-70-14-24',\n",
    "                 'wikipedia-6017-8-20-26-4-7']\n",
    "error_label_1 = ['wikitree-55837-4-0-2-10-11']\n",
    "error_label_2 = ['wikitree-62775-3-3-7-0-2']\n",
    "error_label_3 = ['wikipedia-23188-0-74-86-41-42']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1354ff5d-0fbb-43d3-b5fe-92ed9bccb033",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7f4b614c1550>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "config.num_labels = 42\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config)\n",
    "model = model.to(device)\n",
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "316bac3d-c94e-43fe-92fd-2277ceeba1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_xlm_roberta.XLMRobertaTokenizer'>\n",
      "<class 'transformers.configuration_xlm_roberta.XLMRobertaConfig'>\n",
      "<class 'transformers.modeling_xlm_roberta.XLMRobertaForSequenceClassification'>\n"
     ]
    }
   ],
   "source": [
    "# check everything is fine\n",
    "print(type(tokenizer))\n",
    "print(type(config))\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3bb76b-256a-4827-8105-84ecf31ee504",
   "metadata": {},
   "source": [
    "### 1. Dataset & DataLoader 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5c9924-af61-4aa0-8986-a7c8695387af",
   "metadata": {},
   "source": [
    "**add_entity_tokens**\n",
    "- input\n",
    "    - entity token을 추가할 문장\n",
    "    - 첫 번째 entity 시작, 끝 index\n",
    "    - 두 번째 entity 시작, 끝 index\n",
    "- output\n",
    "    - 해당하는 index에 entity token이 추가된 문장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63286f78-b139-415d-bce6-bcbe2d487eb9",
   "metadata": {},
   "source": [
    "**make_embedding_layer**\n",
    "- input\n",
    "    - 문장의 input_ids\n",
    "- output\n",
    "    - entity에 해당하는 token이면 1, 아니면 0으로 나타내는 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "583f5ad7-635f-4f8c-b779-4e9f00a6a0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embedding_layer(input_ids):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    flag = False\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    special_tokens = special_tokens_dict['additional_special_tokens']\n",
    "    is_entity_layer = []\n",
    "\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token in special_tokens:\n",
    "            if flag == False:\n",
    "                flag = True\n",
    "            else:\n",
    "                flag = False\n",
    "        else:\n",
    "            if flag == True:\n",
    "                is_entity_layer.append(5)\n",
    "                continue\n",
    "        is_entity_layer.append(0)\n",
    "\n",
    "    is_entity_layer = torch.tensor(is_entity_layer)\n",
    "    return is_entity_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "05f6409f-b16f-403b-8512-d603da87f14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_entity_tokens(sentence, a1, a2, b1, b2):\n",
    "    new_sentence = None\n",
    "    # special_tokens = special_tokens_dict['additional_special_tokens']\n",
    "    \n",
    "    if a1 > b1: # b1 먼저\n",
    "        # new_sentence = sentence[:b1] + special_tokens[2] + sentence[b1:b2+1] + special_tokens[3] + sentence[b2+1:a1] + special_tokens[0] + sentence[a1:a2+1] + special_tokens[1] + sentence[a2+1:]\n",
    "        new_sentence = sentence[:b1] + \"$\" + sentence[b1:b2+1] + \"$\" + sentence[b2+1:a1] + \"#\" + sentence[a1:a2+1] + \"#\" + sentence[a2+1:]\n",
    "    else: # a1 먼저\n",
    "        new_sentence = sentence[:a1] + \"#\" + sentence[a1:a2+1] + \"#\" + sentence[a2+1:b1] + \"$\" + sentence[b1:b2+1] + \"$\" + sentence[b2+1:]\n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "01f23361-2dca-4d5b-8e1f-79149511f4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_dir, add_entity=True):\n",
    "    with open('/opt/ml/input/data/label_type.pkl', 'rb') as f:\n",
    "        label_type = pickle.load(f)\n",
    "    dataset = pd.read_csv(dataset_dir, delimiter='\\t', header=None)\n",
    "    dataset = preprocessing_dataset(dataset, label_type, add_entity)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def preprocessing_dataset(dataset, label_type, add_entity):\n",
    "    label = []\n",
    "    sentences = None\n",
    "    \"\"\"\n",
    "    for i in dataset[8]:\n",
    "        if i == 'blind':\n",
    "            label.append(100)\n",
    "        else:\n",
    "            label.append(label_type[i])\n",
    "    \"\"\"\n",
    "    for ID, i in zip(dataset[0], dataset[8]):\n",
    "        if i == 'blind':\n",
    "            label.append(100)\n",
    "        elif ID in error_label_0:\n",
    "            label.append(label_type['관계_없음'])\n",
    "        elif ID in error_label_1:\n",
    "            label.append(label_type['단체:구성원'])\n",
    "        elif ID in error_label_2:\n",
    "            label.append(label_type['단체:본사_도시'])\n",
    "        elif ID in error_label_3:\n",
    "            label.append(label_type['단체:하위_단체'])\n",
    "        else:\n",
    "            label.append(label_type[i])\n",
    "    \n",
    "    if add_entity:\n",
    "        ### 이 부분을 더 효율적으로 고치려면???\n",
    "        sentences = [add_entity_tokens(dataset[1][i], dataset[3][i], dataset[4][i], dataset[6][i], dataset[7][i]) for i in range(len(dataset))]\n",
    "    else:\n",
    "        sentences = dataset[1]\n",
    "\n",
    "    out_dataset = pd.DataFrame({'sentence':sentences,'entity_01':dataset[2],'entity_02':dataset[5],'label':label,})\n",
    "    return out_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8257aa83-25d8-4eec-81f4-54a5bff061e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_UNK_token = True\n",
    "add_ENT_token = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fa8affa9-edc7-4e69-aeb7-0e7340691943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizing_dataset(dataset, tokenizer):\n",
    "    added_token_num = 0\n",
    "    if add_UNK_token:\n",
    "        for text in list(dataset['sentence']):\n",
    "            input_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "            decoded_ids = tokenizer.decode(input_ids)\n",
    "\n",
    "            ori_text = ''.join(text.split(' '))\n",
    "            dec_text = ''.join(decoded_ids.split(' '))\n",
    "            if ori_text == dec_text: continue\n",
    "\n",
    "            unk_cha = ''\n",
    "            unk_list = []\n",
    "            for dec in list(dec_text.split('[UNK]')):\n",
    "                if dec == '': continue\n",
    "                ori_text = list(ori_text.split(dec))\n",
    "                unk_cha = ori_text[0]\n",
    "                if unk_cha != '':\n",
    "                    unk_list.append(unk_cha)\n",
    "                    unk_cha = ''\n",
    "                ori_text = ''.join(ori_text[1:])\n",
    "                if ori_text == '': break\n",
    "            added_token_num += tokenizer.add_tokens(list(''.join(unk_list)))\n",
    "    if add_ENT_token:\n",
    "        special_tokens_dict = {'additional_special_tokens': ['[ENT]']}\n",
    "        added_token_num += tokenizer.add_special_tokens(special_tokens_dict)\n",
    "        \n",
    "    concat_entity = []\n",
    "    for e01, e02 in zip(dataset['entity_01'], dataset['entity_02']):\n",
    "        temp = e01 + '[SEP][SEP]' + e02 # 태양님 추천\n",
    "        concat_entity.append(temp)\n",
    "\n",
    "    tokenized_dataset = tokenizer(concat_entity,\n",
    "                                  list(dataset['sentence']),\n",
    "                                  return_tensors=\"pt\",\n",
    "                                  padding=True,\n",
    "                                  truncation=True,\n",
    "                                  max_length=190,\n",
    "                                  add_special_tokens=True)\n",
    "    return tokenized_dataset, added_token_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "683a16c7-3d6a-4e39-aaf6-fdacdfdf0d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KoElecDataset(Dataset):\n",
    "    def __init__(self, tsv_file, add_entity=True, handle_UNK='REMOVE'):\n",
    "        self.dataset = load_data(tsv_file, add_entity)\n",
    "        self.dataset['sentence'] = self.dataset['entity_01'] + ' [SEP] ' + self.dataset['entity_02'] + ' [SEP] ' + self.dataset['sentence']\n",
    "        self.sentences = list(self.dataset['sentence'])\n",
    "        self.labels = list(self.dataset['label'])\n",
    "        self.tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "        self.tokenizer.add_special_tokens(special_tokens_dict)\n",
    "        self.handle_UNK = handle_UNK\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sentence, label = self.sentences[idx], self.labels[idx]\n",
    "        inputs = self.tokenizer(\n",
    "            sentence,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            max_length=190,\n",
    "            pad_to_max_length=True,\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "            \n",
    "        input_ids = inputs['input_ids'][0]\n",
    "        is_embedding_layer = make_embedding_layer(input_ids, self.tokenizer)\n",
    "        attention_mask = inputs['attention_mask'][0] + is_embedding_layer\n",
    "        \n",
    "        return input_ids, attention_mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a3530fd1-40ad-4541-b791-f210d36035b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KlueDataset(Dataset):\n",
    "    def __init__(self, tokenized_dataset, labels):\n",
    "        self.tokenized_dataset = tokenized_dataset\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.tokenized_dataset.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        is_embedding_layer = make_embedding_layer(item['input_ids'])\n",
    "        item['attention_mask'] = item['attention_mask'] + is_embedding_layer\n",
    "        # item['input_ids'], item['attention_mask'], item['labels']\n",
    "        return item['input_ids'], item['attention_mask'], item['labels']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ef21bda9-ffc4-4805-9e24-cbc97db3962b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We need to remove 26 to truncate the inputbut the first sequence has a length 12. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 89 to truncate the inputbut the first sequence has a length 15. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 54 to truncate the inputbut the first sequence has a length 13. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 30 to truncate the inputbut the first sequence has a length 17. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 35 to truncate the inputbut the first sequence has a length 14. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 18 to truncate the inputbut the first sequence has a length 11. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 55 to truncate the inputbut the first sequence has a length 16. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 16 to truncate the inputbut the first sequence has a length 13. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 13 to truncate the inputbut the first sequence has a length 13. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 44 to truncate the inputbut the first sequence has a length 23. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 75 to truncate the inputbut the first sequence has a length 13. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 108 to truncate the inputbut the first sequence has a length 21. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 36 to truncate the inputbut the first sequence has a length 17. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 30 to truncate the inputbut the first sequence has a length 9. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 18 to truncate the inputbut the first sequence has a length 10. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 19 to truncate the inputbut the first sequence has a length 18. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 53 to truncate the inputbut the first sequence has a length 13. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 41 to truncate the inputbut the first sequence has a length 12. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 30 to truncate the inputbut the first sequence has a length 17. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 51 to truncate the inputbut the first sequence has a length 16. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 18 to truncate the inputbut the first sequence has a length 18. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 28 to truncate the inputbut the first sequence has a length 16. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 98 to truncate the inputbut the first sequence has a length 14. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 33 to truncate the inputbut the first sequence has a length 12. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 69 to truncate the inputbut the first sequence has a length 12. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 31 to truncate the inputbut the first sequence has a length 12. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 24 to truncate the inputbut the first sequence has a length 13. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 63 to truncate the inputbut the first sequence has a length 16. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 47 to truncate the inputbut the first sequence has a length 15. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 18 to truncate the inputbut the first sequence has a length 18. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 45 to truncate the inputbut the first sequence has a length 12. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 23 to truncate the inputbut the first sequence has a length 10. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 39 to truncate the inputbut the first sequence has a length 10. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 47 to truncate the inputbut the first sequence has a length 15. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 62 to truncate the inputbut the first sequence has a length 16. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 75 to truncate the inputbut the first sequence has a length 13. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 90 to truncate the inputbut the first sequence has a length 15. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 35 to truncate the inputbut the first sequence has a length 14. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 74 to truncate the inputbut the first sequence has a length 10. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 75 to truncate the inputbut the first sequence has a length 15. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 41 to truncate the inputbut the first sequence has a length 16. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 23 to truncate the inputbut the first sequence has a length 17. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 24 to truncate the inputbut the first sequence has a length 13. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 103 to truncate the inputbut the first sequence has a length 14. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 114 to truncate the inputbut the first sequence has a length 16. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 72 to truncate the inputbut the first sequence has a length 12. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 45 to truncate the inputbut the first sequence has a length 12. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 175 to truncate the inputbut the first sequence has a length 18. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 70 to truncate the inputbut the first sequence has a length 16. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 44 to truncate the inputbut the first sequence has a length 17. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 138 to truncate the inputbut the first sequence has a length 14. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 55 to truncate the inputbut the first sequence has a length 15. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 48 to truncate the inputbut the first sequence has a length 15. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 32 to truncate the inputbut the first sequence has a length 11. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(250002, 1024)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_data(os.path.join(train_dir, 'train.tsv'))\n",
    "labels = dataset['label'].values\n",
    "\n",
    "tokenized_dataset, added_token_num = tokenizing_dataset(dataset, tokenizer)\n",
    "klue_dataset = KlueDataset(tokenized_dataset, labels)\n",
    "\n",
    "model.resize_token_embeddings(tokenizer.vocab_size + added_token_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "55b3522c-d951-4cd3-96d7-19bee23c7dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We need to remove 20 to truncate the inputbut the first sequence has a length 19. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 23 to truncate the inputbut the first sequence has a length 16. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n",
      "We need to remove 53 to truncate the inputbut the first sequence has a length 12. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.\n"
     ]
    }
   ],
   "source": [
    "def load_test_dataset(dataset_dir, tokenizer):\n",
    "    test_dataset = load_data(dataset_dir)\n",
    "    test_label = test_dataset['label'].values\n",
    "    tokenized_test = tokenizing_dataset(test_dataset, tokenizer)\n",
    "    return tokenized_test, test_label\n",
    "\n",
    "test_dataset, test_label = load_test_dataset(os.path.join(test_dir, 'test.tsv'), tokenizer)\n",
    "test_dataset = KlueDataset(test_dataset, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bad6ef-5713-47f2-84f5-fbd3a8c92265",
   "metadata": {},
   "source": [
    "**Train, Valid set 8 : 2 로 분리**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b3efc4f-3677-46a6-ab7d-d4b25ca74680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset, val_dataset = train_test_split(dataset, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "182770d7-d6b7-46a1-8925-d0f560ac78c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b3cbc093-3346-4e1c-9b16-73e3170630b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=5)\n",
    "#val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=5)\n",
    "train_loader = DataLoader(klue_dataset, batch_size=batch_size, num_workers=5, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3f0cdbfb-f781-4b00-ba33-79f3fd64c634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7a6dca5f-c790-4edd-a677-45e5d37f2d4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 185, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-49-8cedcc0bed95>\", line 9, in __getitem__\n    is_embedding_layer = make_embedding_layer(item['input_ids'])\n  File \"<ipython-input-43-a07466dad1f1>\", line 4, in make_embedding_layer\n    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n  File \"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils.py\", line 859, in convert_ids_to_tokens\n    tokens.append(self._convert_id_to_token(index))\n  File \"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_xlm_roberta.py\", line 285, in _convert_id_to_token\n    return self.sp_model.IdToPiece(index - self.fairseq_offset)\n  File \"/opt/conda/lib/python3.7/site-packages/sentencepiece/__init__.py\", line 501, in _batched_func\n    return _func(self, arg)\n  File \"/opt/conda/lib/python3.7/site-packages/sentencepiece/__init__.py\", line 494, in _func\n    raise IndexError('piece id is out of range.')\nIndexError: piece id is out of range.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-0fd2476fd2ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 185, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-49-8cedcc0bed95>\", line 9, in __getitem__\n    is_embedding_layer = make_embedding_layer(item['input_ids'])\n  File \"<ipython-input-43-a07466dad1f1>\", line 4, in make_embedding_layer\n    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n  File \"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils.py\", line 859, in convert_ids_to_tokens\n    tokens.append(self._convert_id_to_token(index))\n  File \"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_xlm_roberta.py\", line 285, in _convert_id_to_token\n    return self.sp_model.IdToPiece(index - self.fairseq_offset)\n  File \"/opt/conda/lib/python3.7/site-packages/sentencepiece/__init__.py\", line 501, in _batched_func\n    return _func(self, arg)\n  File \"/opt/conda/lib/python3.7/site-packages/sentencepiece/__init__.py\", line 494, in _func\n    raise IndexError('piece id is out of range.')\nIndexError: piece id is out of range.\n"
     ]
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd31d4ea-ca3b-41a9-9ab0-2e337e15af30",
   "metadata": {},
   "source": [
    "### 2. Model 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0039622-dab3-49a8-ab92-da193b2ea4db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'electra.embeddings.position_ids']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = ElectraForSequenceClassification.from_pretrained(\"monologg/koelectra-base-v3-discriminator\", num_labels=42).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1b5680f-de99-4b49-81b2-93be56415040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(35004, 768)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(35004) # 내가 임의로 입력함 나중에 수정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a351c37e-f09c-4497-bf77-3dc2b6bcb093",
   "metadata": {},
   "source": [
    "### 3. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7efb6834-66c3-4477-903e-3b1399895718",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "ls_loss = LabelSmoothingLoss()\n",
    "cels_loss = CELSLoss()\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6) # 익효님꺼로 파라미터 변경\n",
    "# scheduler = CosineAnnealingLR(optimizer, T_max=2, eta_min=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63757f1d-9b2c-4d7b-9cb8-12dbdc47f109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d05d95b5-48df-420e-bba7-24f6c77d2982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a05570c156648aa85466f168c702791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 7.272258758544922 train acc 0.1875\n",
      "epoch 1 batch id 51 loss 6.673955917358398 train acc 0.39705882352941174\n",
      "epoch 1 batch id 101 loss 6.026796340942383 train acc 0.42636138613861385\n",
      "epoch 1 batch id 151 loss 4.209381103515625 train acc 0.44867549668874174\n",
      "epoch 1 batch id 201 loss 5.922874450683594 train acc 0.45677860696517414\n",
      "epoch 1 batch id 251 loss 4.489680290222168 train acc 0.4663844621513944\n",
      "epoch 1 batch id 301 loss 8.037888526916504 train acc 0.4688538205980066\n",
      "epoch 1 batch id 351 loss 5.937761306762695 train acc 0.47150997150997154\n",
      "epoch 1 batch id 401 loss 3.1563119888305664 train acc 0.47989401496259354\n",
      "epoch 1 batch id 451 loss 8.230195999145508 train acc 0.48822062084257206\n",
      "epoch 1 batch id 501 loss 4.939570903778076 train acc 0.4806636726546906\n",
      "epoch 1 batch id 551 loss 3.7603254318237305 train acc 0.48139745916515425\n",
      "epoch 1 batch id 601 loss 5.61888313293457 train acc 0.47878535773710484\n",
      "epoch 1 batch id 651 loss 5.980191707611084 train acc 0.48473502304147464\n",
      "epoch 1 batch id 701 loss 5.540711402893066 train acc 0.48493223965763194\n",
      "epoch 1 batch id 751 loss 5.957098484039307 train acc 0.48418774966711053\n",
      "epoch 1 batch id 801 loss 1.7715353965759277 train acc 0.48174157303370785\n",
      "epoch 1 batch id 851 loss 4.343622207641602 train acc 0.478334312573443\n",
      "epoch 1 batch id 901 loss 5.59278678894043 train acc 0.4820338512763596\n",
      "epoch 1 batch id 951 loss 3.6367063522338867 train acc 0.47864090431125134\n",
      "epoch 1 batch id 1001 loss 6.747197151184082 train acc 0.4772102897102897\n",
      "epoch 1 batch id 1051 loss 2.993542194366455 train acc 0.48126784015223595\n",
      "epoch 1 batch id 1101 loss 2.782803535461426 train acc 0.4858083560399637\n",
      "epoch 1 batch id 1151 loss 4.87271785736084 train acc 0.4853388357949609\n",
      "epoch 1 batch id 1201 loss 5.273197174072266 train acc 0.4833992506244796\n",
      "epoch 1 batch id 1251 loss 4.325986862182617 train acc 0.486810551558753\n",
      "epoch 1 batch id 1301 loss 2.4759883880615234 train acc 0.4870292083013067\n",
      "epoch 1 batch id 1351 loss 4.398199081420898 train acc 0.4877868245743893\n",
      "epoch 1 batch id 1401 loss 3.8243753910064697 train acc 0.4886688079942898\n",
      "epoch 1 batch id 1451 loss 3.9630115032196045 train acc 0.48815472088215023\n",
      "epoch 1 batch id 1501 loss 8.746145248413086 train acc 0.4887574950033311\n",
      "epoch 1 batch id 1551 loss 5.134233474731445 train acc 0.48952288845905867\n",
      "epoch 1 batch id 1601 loss 6.057308197021484 train acc 0.491762960649594\n",
      "epoch 1 batch id 1651 loss 2.7154695987701416 train acc 0.49068746214415504\n",
      "epoch 1 batch id 1701 loss 5.2528791427612305 train acc 0.49107142857142855\n",
      "epoch 1 batch id 1751 loss 1.1188690662384033 train acc 0.49171901770416904\n",
      "epoch 1 batch id 1801 loss 5.967029571533203 train acc 0.49285119378123265\n",
      "epoch 1 batch id 1851 loss 3.9272866249084473 train acc 0.49409103187466236\n",
      "epoch 1 batch id 1901 loss 5.141287803649902 train acc 0.49572593371909524\n",
      "epoch 1 batch id 1951 loss 2.6496782302856445 train acc 0.49503459764223473\n",
      "epoch 1 batch id 2001 loss 2.0492002964019775 train acc 0.49650174912543726\n",
      "epoch 1 batch id 2051 loss 0.8187885284423828 train acc 0.49832398829839103\n",
      "epoch 1 batch id 2101 loss 3.2958037853240967 train acc 0.49866135173726794\n",
      "epoch 1 batch id 2151 loss 2.6793947219848633 train acc 0.5003486750348675\n",
      "epoch 1 batch id 2201 loss 1.9192910194396973 train acc 0.5012778282598819\n",
      "epoch 1 batch id 2251 loss 3.3352127075195312 train acc 0.5034429142603287\n",
      "epoch 1 batch id 2301 loss 1.4992351531982422 train acc 0.5034224250325945\n",
      "epoch 1 batch id 2351 loss 0.9962623119354248 train acc 0.5046788600595491\n",
      "epoch 1 batch id 2401 loss 1.2876280546188354 train acc 0.5050239483548521\n",
      "epoch 1 batch id 2451 loss 4.397339820861816 train acc 0.5057884536923705\n",
      "epoch 1 batch id 2501 loss 1.9296989440917969 train acc 0.5065723710515794\n",
      "epoch 1 batch id 2551 loss 1.4418017864227295 train acc 0.5070070560564485\n",
      "epoch 1 batch id 2601 loss 1.1117615699768066 train acc 0.5088187235678585\n",
      "epoch 1 batch id 2651 loss 3.273433208465576 train acc 0.5103970199924557\n",
      "epoch 1 batch id 2701 loss 4.966363906860352 train acc 0.5116623472787857\n",
      "epoch 1 batch id 2751 loss 2.514096975326538 train acc 0.5124272991639404\n",
      "epoch 1 batch id 2801 loss 2.2045750617980957 train acc 0.51454837558015\n",
      "\n",
      "epoch 1 train acc 0.514975115535016\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0de3dbd685f44fc2a6d775088d686771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 3.7403199672698975 train acc 0.3125\n",
      "epoch 2 batch id 51 loss 1.5157744884490967 train acc 0.6029411764705882\n",
      "epoch 2 batch id 101 loss 3.796792984008789 train acc 0.556930693069307\n",
      "epoch 2 batch id 151 loss 1.2219057083129883 train acc 0.5620860927152318\n",
      "epoch 2 batch id 201 loss 3.876227378845215 train acc 0.5699626865671642\n",
      "epoch 2 batch id 251 loss 1.7792237997055054 train acc 0.5864043824701195\n",
      "epoch 2 batch id 301 loss 6.283151149749756 train acc 0.5820182724252492\n",
      "epoch 2 batch id 351 loss 5.178122520446777 train acc 0.5888532763532763\n",
      "epoch 2 batch id 401 loss 1.028862476348877 train acc 0.5897755610972568\n",
      "epoch 2 batch id 451 loss 6.330960273742676 train acc 0.6004711751662971\n",
      "epoch 2 batch id 501 loss 1.6671016216278076 train acc 0.5993013972055888\n",
      "epoch 2 batch id 551 loss 3.046321392059326 train acc 0.6012931034482759\n",
      "epoch 2 batch id 601 loss 3.7662477493286133 train acc 0.598585690515807\n",
      "epoch 2 batch id 651 loss 4.479378700256348 train acc 0.6037826420890937\n",
      "epoch 2 batch id 701 loss 4.161983489990234 train acc 0.6037803138373752\n",
      "epoch 2 batch id 751 loss 4.503307342529297 train acc 0.6071071904127829\n",
      "epoch 2 batch id 801 loss 1.5549838542938232 train acc 0.6022940074906367\n",
      "epoch 2 batch id 851 loss 1.9612035751342773 train acc 0.6046562867215041\n",
      "epoch 2 batch id 901 loss 4.379605293273926 train acc 0.608906770255272\n",
      "epoch 2 batch id 951 loss 1.3875453472137451 train acc 0.6094242902208202\n",
      "epoch 2 batch id 1001 loss 4.561308860778809 train acc 0.6106393606393606\n",
      "epoch 2 batch id 1051 loss 2.3640661239624023 train acc 0.6127497621313035\n",
      "epoch 2 batch id 1101 loss 3.7975871562957764 train acc 0.6176771117166212\n",
      "epoch 2 batch id 1151 loss 2.8554325103759766 train acc 0.6190269331016507\n",
      "epoch 2 batch id 1201 loss 3.1192331314086914 train acc 0.6195358034970858\n",
      "epoch 2 batch id 1251 loss 2.555361747741699 train acc 0.6229016786570744\n",
      "epoch 2 batch id 1301 loss 2.083073377609253 train acc 0.6274500384319754\n",
      "epoch 2 batch id 1351 loss 2.440016746520996 train acc 0.6280995558845299\n",
      "epoch 2 batch id 1401 loss 2.005702018737793 train acc 0.6313793718772306\n",
      "epoch 2 batch id 1451 loss 1.6249767541885376 train acc 0.6313318401102688\n",
      "epoch 2 batch id 1501 loss 7.714206695556641 train acc 0.6319953364423717\n",
      "epoch 2 batch id 1551 loss 2.791836738586426 train acc 0.6335428755641521\n",
      "epoch 2 batch id 1601 loss 3.1768393516540527 train acc 0.6352670206121174\n",
      "epoch 2 batch id 1651 loss 1.4337353706359863 train acc 0.6358267716535433\n",
      "epoch 2 batch id 1701 loss 2.7833399772644043 train acc 0.6374559082892416\n",
      "epoch 2 batch id 1751 loss 0.8864277005195618 train acc 0.6380282695602513\n",
      "epoch 2 batch id 1801 loss 4.782690525054932 train acc 0.6389158800666297\n",
      "epoch 2 batch id 1851 loss 3.2206926345825195 train acc 0.6404646137223122\n",
      "epoch 2 batch id 1901 loss 4.803400993347168 train acc 0.6432140978432404\n",
      "epoch 2 batch id 1951 loss 1.6458547115325928 train acc 0.6436442849820605\n",
      "epoch 2 batch id 2001 loss 0.8639416694641113 train acc 0.6449900049975013\n",
      "epoch 2 batch id 2051 loss 0.25284215807914734 train acc 0.6470319356411507\n",
      "epoch 2 batch id 2101 loss 2.737649440765381 train acc 0.646834840552118\n",
      "epoch 2 batch id 2151 loss 1.218005895614624 train acc 0.6481578335657834\n",
      "epoch 2 batch id 2201 loss 0.5045245885848999 train acc 0.6481428895956384\n",
      "epoch 2 batch id 2251 loss 2.350452423095703 train acc 0.6493225233229676\n",
      "epoch 2 batch id 2301 loss 0.5105966329574585 train acc 0.650396566710126\n",
      "epoch 2 batch id 2351 loss 0.4609847664833069 train acc 0.6513983411314335\n",
      "epoch 2 batch id 2401 loss 0.285614013671875 train acc 0.6524364847980009\n",
      "epoch 2 batch id 2451 loss 3.27738094329834 train acc 0.6518767849857201\n",
      "epoch 2 batch id 2501 loss 1.5005383491516113 train acc 0.6523890443822471\n",
      "epoch 2 batch id 2551 loss 0.7546353936195374 train acc 0.6510927087416699\n",
      "epoch 2 batch id 2601 loss 0.43635040521621704 train acc 0.6521289888504421\n",
      "epoch 2 batch id 2651 loss 1.9911137819290161 train acc 0.6539749151263674\n",
      "epoch 2 batch id 2701 loss 3.190365791320801 train acc 0.6545723805997778\n",
      "epoch 2 batch id 2751 loss 1.5057765245437622 train acc 0.6544211195928753\n",
      "epoch 2 batch id 2801 loss 1.433241844177246 train acc 0.6551008568368439\n",
      "\n",
      "epoch 2 train acc 0.654950231070032\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3619d7d6bd14e7ea3834feca1338e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 2.4970407485961914 train acc 0.4375\n",
      "epoch 3 batch id 51 loss 0.7761947512626648 train acc 0.6678921568627451\n",
      "epoch 3 batch id 101 loss 3.484156847000122 train acc 0.6398514851485149\n",
      "epoch 3 batch id 151 loss 0.47594958543777466 train acc 0.6485927152317881\n",
      "epoch 3 batch id 201 loss 2.02579402923584 train acc 0.6598258706467661\n",
      "epoch 3 batch id 251 loss 0.9196346402168274 train acc 0.6710657370517928\n",
      "epoch 3 batch id 301 loss 5.404382705688477 train acc 0.668812292358804\n",
      "epoch 3 batch id 351 loss 3.343883514404297 train acc 0.6785968660968661\n",
      "epoch 3 batch id 401 loss 0.2589179277420044 train acc 0.6769014962593516\n",
      "epoch 3 batch id 451 loss 5.069751739501953 train acc 0.6838968957871396\n",
      "epoch 3 batch id 501 loss 0.5187137126922607 train acc 0.685379241516966\n",
      "epoch 3 batch id 551 loss 2.507948160171509 train acc 0.6855716878402904\n",
      "epoch 3 batch id 601 loss 2.098541736602783 train acc 0.6815723793677204\n",
      "epoch 3 batch id 651 loss 3.6151535511016846 train acc 0.6872119815668203\n",
      "epoch 3 batch id 701 loss 3.2698934078216553 train acc 0.6857168330955777\n",
      "epoch 3 batch id 751 loss 4.200188636779785 train acc 0.6889980026631158\n",
      "epoch 3 batch id 801 loss 0.37740030884742737 train acc 0.6846129837702871\n",
      "epoch 3 batch id 851 loss 0.7999494075775146 train acc 0.6880141010575793\n",
      "epoch 3 batch id 901 loss 3.728208541870117 train acc 0.6899972253052165\n",
      "epoch 3 batch id 951 loss 0.7171337008476257 train acc 0.6905888538380652\n",
      "epoch 3 batch id 1001 loss 2.1070938110351562 train acc 0.6912462537462537\n",
      "epoch 3 batch id 1051 loss 2.0422372817993164 train acc 0.6908896289248335\n",
      "epoch 3 batch id 1101 loss 3.1505022048950195 train acc 0.6947661217075386\n",
      "epoch 3 batch id 1151 loss 1.9438025951385498 train acc 0.6966768027801912\n",
      "epoch 3 batch id 1201 loss 2.109867811203003 train acc 0.6968151540383014\n",
      "epoch 3 batch id 1251 loss 2.0362300872802734 train acc 0.699740207833733\n",
      "epoch 3 batch id 1301 loss 1.7269151210784912 train acc 0.7032090699461953\n",
      "epoch 3 batch id 1351 loss 2.0153491497039795 train acc 0.7035066617320503\n",
      "epoch 3 batch id 1401 loss 0.7725263237953186 train acc 0.7065488936473947\n",
      "epoch 3 batch id 1451 loss 0.387260377407074 train acc 0.7058494141971055\n",
      "epoch 3 batch id 1501 loss 6.426794052124023 train acc 0.7063624250499667\n",
      "epoch 3 batch id 1551 loss 1.4758453369140625 train acc 0.7089377820760799\n",
      "epoch 3 batch id 1601 loss 2.2589125633239746 train acc 0.7102592129918801\n",
      "epoch 3 batch id 1651 loss 1.0831866264343262 train acc 0.7103270745003029\n",
      "epoch 3 batch id 1701 loss 1.2147157192230225 train acc 0.7122648442092887\n",
      "epoch 3 batch id 1751 loss 1.5009640455245972 train acc 0.7120573957738435\n",
      "epoch 3 batch id 1801 loss 3.406541109085083 train acc 0.7135966129927818\n",
      "epoch 3 batch id 1851 loss 2.592561721801758 train acc 0.7150189086980011\n",
      "epoch 3 batch id 1901 loss 4.351426124572754 train acc 0.7164321409784324\n",
      "epoch 3 batch id 1951 loss 1.030996561050415 train acc 0.7181573552024603\n",
      "epoch 3 batch id 2001 loss 0.16322220861911774 train acc 0.7186094452773614\n",
      "epoch 3 batch id 2051 loss 0.3076377809047699 train acc 0.7200755728912726\n",
      "epoch 3 batch id 2101 loss 2.154104709625244 train acc 0.7198357924797716\n",
      "epoch 3 batch id 2151 loss 0.23657715320587158 train acc 0.7213795908879591\n",
      "epoch 3 batch id 2201 loss 0.2300158143043518 train acc 0.7201272149023171\n",
      "epoch 3 batch id 2251 loss 1.669640302658081 train acc 0.7211239449133718\n",
      "epoch 3 batch id 2301 loss 0.2971597909927368 train acc 0.7219415471534115\n",
      "epoch 3 batch id 2351 loss 0.32378971576690674 train acc 0.7230965546575925\n",
      "epoch 3 batch id 2401 loss 0.11065014451742172 train acc 0.7242034568929613\n",
      "epoch 3 batch id 2451 loss 2.4704208374023438 train acc 0.7241942064463485\n",
      "epoch 3 batch id 2501 loss 0.6165446043014526 train acc 0.7250349860055978\n",
      "epoch 3 batch id 2551 loss 0.5894173383712769 train acc 0.7245197961583693\n",
      "epoch 3 batch id 2601 loss 0.08801668882369995 train acc 0.7246491733948481\n",
      "epoch 3 batch id 2651 loss 1.0878878831863403 train acc 0.7262825348924934\n",
      "epoch 3 batch id 2701 loss 2.3135757446289062 train acc 0.726490188818956\n",
      "epoch 3 batch id 2751 loss 1.0799129009246826 train acc 0.7264858233369684\n",
      "epoch 3 batch id 2801 loss 0.23364511132240295 train acc 0.727351838629061\n",
      "\n",
      "epoch 3 train acc 0.7273151439744046\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de9fc95577154a9792b57a98a48652d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1 loss 1.5283775329589844 train acc 0.8125\n",
      "epoch 4 batch id 51 loss 0.33055874705314636 train acc 0.7463235294117647\n",
      "epoch 4 batch id 101 loss 3.163536548614502 train acc 0.719059405940594\n",
      "epoch 4 batch id 151 loss 0.334639310836792 train acc 0.7272350993377483\n",
      "epoch 4 batch id 201 loss 1.8308120965957642 train acc 0.7307213930348259\n",
      "epoch 4 batch id 251 loss 0.3766354024410248 train acc 0.7355577689243028\n",
      "epoch 4 batch id 301 loss 4.703818321228027 train acc 0.7323504983388704\n",
      "epoch 4 batch id 351 loss 2.7060351371765137 train acc 0.7403846153846154\n",
      "epoch 4 batch id 401 loss 0.06334559619426727 train acc 0.739713216957606\n",
      "epoch 4 batch id 451 loss 3.7516708374023438 train acc 0.7448725055432373\n",
      "epoch 4 batch id 501 loss 0.23402813076972961 train acc 0.747255489021956\n",
      "epoch 4 batch id 551 loss 2.2515430450439453 train acc 0.75\n",
      "epoch 4 batch id 601 loss 1.1276555061340332 train acc 0.7480241264559068\n",
      "epoch 4 batch id 651 loss 3.0977838039398193 train acc 0.7529761904761905\n",
      "epoch 4 batch id 701 loss 3.138505458831787 train acc 0.7526747503566333\n",
      "epoch 4 batch id 751 loss 3.9693832397460938 train acc 0.7554926764314248\n",
      "epoch 4 batch id 801 loss 0.1330225169658661 train acc 0.7532771535580525\n",
      "epoch 4 batch id 851 loss 0.47711431980133057 train acc 0.7555082256169212\n",
      "epoch 4 batch id 901 loss 3.057246685028076 train acc 0.7576997780244173\n",
      "epoch 4 batch id 951 loss 0.4554879665374756 train acc 0.7598580441640379\n",
      "epoch 4 batch id 1001 loss 1.307776927947998 train acc 0.7610514485514486\n",
      "epoch 4 batch id 1051 loss 1.1846563816070557 train acc 0.7620123691722169\n",
      "epoch 4 batch id 1101 loss 1.4384815692901611 train acc 0.7654972752043597\n",
      "epoch 4 batch id 1151 loss 1.3191006183624268 train acc 0.766724587315378\n",
      "epoch 4 batch id 1201 loss 1.7729747295379639 train acc 0.766808909242298\n",
      "epoch 4 batch id 1251 loss 1.4587093591690063 train acc 0.7699840127897681\n",
      "epoch 4 batch id 1301 loss 1.8843728303909302 train acc 0.7720503458877787\n",
      "epoch 4 batch id 1351 loss 1.1448293924331665 train acc 0.7744263508512214\n",
      "epoch 4 batch id 1401 loss 0.39658451080322266 train acc 0.7767665952890792\n",
      "epoch 4 batch id 1451 loss 0.2129001021385193 train acc 0.7773087525844246\n",
      "epoch 4 batch id 1501 loss 4.685647010803223 train acc 0.7784810126582279\n",
      "epoch 4 batch id 1551 loss 0.9914060235023499 train acc 0.7805851063829787\n",
      "epoch 4 batch id 1601 loss 1.3966691493988037 train acc 0.7829091193004373\n",
      "epoch 4 batch id 1651 loss 0.9309802651405334 train acc 0.7832752877044216\n",
      "epoch 4 batch id 1701 loss 1.4466814994812012 train acc 0.78505291005291\n",
      "epoch 4 batch id 1751 loss 2.0045852661132812 train acc 0.7849800114220445\n",
      "epoch 4 batch id 1801 loss 2.485971689224243 train acc 0.785952248750694\n",
      "epoch 4 batch id 1851 loss 1.8379099369049072 train acc 0.7867368989735278\n",
      "epoch 4 batch id 1901 loss 3.8401730060577393 train acc 0.7882035770647028\n",
      "epoch 4 batch id 1951 loss 0.5877885818481445 train acc 0.7894989748846746\n",
      "epoch 4 batch id 2001 loss 0.0863116905093193 train acc 0.7905734632683659\n",
      "epoch 4 batch id 2051 loss 0.3632723093032837 train acc 0.7919612384202828\n",
      "epoch 4 batch id 2101 loss 2.266066551208496 train acc 0.7916170871013803\n",
      "epoch 4 batch id 2151 loss 0.46061885356903076 train acc 0.7929741980474198\n",
      "epoch 4 batch id 2201 loss 0.14612948894500732 train acc 0.7930486142662426\n",
      "epoch 4 batch id 2251 loss 1.6022465229034424 train acc 0.7940637494446913\n",
      "epoch 4 batch id 2301 loss 0.1694173812866211 train acc 0.7943557149065623\n",
      "epoch 4 batch id 2351 loss 0.11557938158512115 train acc 0.7947947681837516\n",
      "epoch 4 batch id 2401 loss 0.2602883279323578 train acc 0.7955018742190754\n",
      "epoch 4 batch id 2451 loss 1.788879632949829 train acc 0.7956191350469196\n",
      "epoch 4 batch id 2501 loss 0.1813541054725647 train acc 0.7955067972810875\n",
      "epoch 5 batch id 201 loss 1.123819351196289 train acc 0.8177860696517413\n",
      "epoch 5 batch id 251 loss 0.16658028960227966 train acc 0.8172310756972112\n",
      "epoch 5 batch id 301 loss 3.8991494178771973 train acc 0.8135382059800664\n",
      "epoch 5 batch id 351 loss 1.704315185546875 train acc 0.8219373219373219\n",
      "epoch 5 batch id 401 loss 0.05031586438417435 train acc 0.8198254364089775\n",
      "epoch 5 batch id 451 loss 2.300485134124756 train acc 0.8226164079822617\n",
      "epoch 5 batch id 501 loss 0.12450568377971649 train acc 0.8246007984031936\n",
      "epoch 5 batch id 551 loss 2.130484104156494 train acc 0.8276996370235935\n",
      "epoch 5 batch id 601 loss 0.4587309658527374 train acc 0.8264351081530782\n",
      "epoch 5 batch id 651 loss 2.823526382446289 train acc 0.8299731182795699\n",
      "epoch 5 batch id 701 loss 2.5880162715911865 train acc 0.8304208273894437\n",
      "epoch 5 batch id 751 loss 3.2214584350585938 train acc 0.8326398135818908\n",
      "epoch 5 batch id 801 loss 0.017234770581126213 train acc 0.8289637952559301\n",
      "epoch 5 batch id 851 loss 0.0922928974032402 train acc 0.8306404230317274\n",
      "epoch 5 batch id 901 loss 2.4175806045532227 train acc 0.8327552719200888\n",
      "epoch 5 batch id 951 loss 0.13374435901641846 train acc 0.8339248159831756\n",
      "epoch 5 batch id 1001 loss 0.4877224266529083 train acc 0.8356643356643356\n",
      "epoch 5 batch id 1051 loss 1.2411683797836304 train acc 0.8356921979067554\n",
      "epoch 5 batch id 1101 loss 2.1644227504730225 train acc 0.837874659400545\n",
      "epoch 5 batch id 1151 loss 0.7763902544975281 train acc 0.8393788010425717\n",
      "epoch 5 batch id 1201 loss 1.3990002870559692 train acc 0.8390924229808493\n",
      "epoch 5 batch id 1251 loss 0.9831820130348206 train acc 0.8409772182254197\n",
      "epoch 5 batch id 1301 loss 1.1807210445404053 train acc 0.8428612605687933\n",
      "epoch 5 batch id 1351 loss 0.634381890296936 train acc 0.8434492968171725\n",
      "epoch 5 batch id 1401 loss 0.1918567419052124 train acc 0.8447537473233405\n",
      "epoch 5 batch id 1451 loss 0.07199296355247498 train acc 0.8449776016540317\n",
      "epoch 5 batch id 1501 loss 3.8572592735290527 train acc 0.8456445702864757\n",
      "epoch 5 batch id 1551 loss 1.0634211301803589 train acc 0.8470744680851063\n",
      "epoch 5 batch id 1601 loss 0.8794106245040894 train acc 0.8488054341036851\n",
      "epoch 5 batch id 1651 loss 0.9824013710021973 train acc 0.8482359176256814\n",
      "epoch 5 batch id 1701 loss 1.0830328464508057 train acc 0.8496105232216343\n",
      "epoch 5 batch id 1751 loss 1.7937655448913574 train acc 0.8495502569960023\n",
      "epoch 5 batch id 1801 loss 1.346120834350586 train acc 0.8500138811771238\n",
      "epoch 5 batch id 1851 loss 1.5083181858062744 train acc 0.8502498649378715\n",
      "epoch 5 batch id 1901 loss 3.350879669189453 train acc 0.850867964229353\n",
      "epoch 5 batch id 1951 loss 0.05168743431568146 train acc 0.8522232188621219\n",
      "epoch 5 batch id 2001 loss 0.048318374902009964 train acc 0.8525424787606197\n",
      "epoch 5 batch id 2051 loss 0.2596871256828308 train acc 0.8529071184787909\n",
      "epoch 5 batch id 2101 loss 1.8876603841781616 train acc 0.8526891956211328\n",
      "epoch 5 batch id 2151 loss 0.23802226781845093 train acc 0.8534983728498373\n",
      "epoch 5 batch id 2201 loss 0.03191109746694565 train acc 0.8532201272149024\n",
      "epoch 5 batch id 2251 loss 2.0733249187469482 train acc 0.8530375388716126\n",
      "epoch 5 batch id 2301 loss 0.0708240270614624 train acc 0.8534332898739678\n",
      "epoch 5 batch id 2351 loss 0.1727847307920456 train acc 0.853945129732029\n",
      "epoch 5 batch id 2401 loss 0.03186771273612976 train acc 0.8545918367346939\n",
      "epoch 5 batch id 2451 loss 1.3137140274047852 train acc 0.854141166870665\n",
      "epoch 5 batch id 2501 loss 0.12279962003231049 train acc 0.8533836465413834\n",
      "epoch 5 batch id 2551 loss 0.23076093196868896 train acc 0.8530478243825951\n",
      "epoch 5 batch id 2601 loss 0.32145023345947266 train acc 0.8526528258362168\n",
      "epoch 5 batch id 2651 loss 0.3081544041633606 train acc 0.8540409279517164\n",
      "epoch 5 batch id 2701 loss 1.833770751953125 train acc 0.8537347278785635\n",
      "epoch 5 batch id 2751 loss 0.4968593120574951 train acc 0.8535532533624137\n",
      "epoch 5 batch id 2801 loss 0.17682665586471558 train acc 0.8539360942520529\n",
      "\n",
      "epoch 5 train acc 0.8540926057589762\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd40556c89d44bb2af74fe1a55168583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 batch id 1 loss 0.3223203718662262 train acc 1.0\n",
      "epoch 6 batch id 51 loss 0.10573983937501907 train acc 0.8725490196078431\n",
      "epoch 6 batch id 101 loss 2.3689968585968018 train acc 0.8508663366336634\n",
      "epoch 6 batch id 151 loss 0.15261194109916687 train acc 0.8497516556291391\n",
      "epoch 6 batch id 201 loss 1.1380054950714111 train acc 0.8479477611940298\n",
      "epoch 6 batch id 251 loss 0.14735965430736542 train acc 0.8523406374501992\n",
      "epoch 6 batch id 301 loss 1.8486146926879883 train acc 0.8515365448504983\n",
      "epoch 6 batch id 351 loss 1.5735357999801636 train acc 0.8575498575498576\n",
      "epoch 6 batch id 401 loss 0.036719661206007004 train acc 0.8559850374064838\n",
      "epoch 6 batch id 451 loss 1.0059475898742676 train acc 0.8607261640798226\n",
      "epoch 6 batch id 501 loss 0.049501001834869385 train acc 0.8627744510978044\n",
      "epoch 6 batch id 551 loss 0.42797160148620605 train acc 0.8651315789473685\n",
      "epoch 6 batch id 601 loss 0.31830114126205444 train acc 0.8632487520798668\n",
      "epoch 6 batch id 651 loss 2.948869228363037 train acc 0.8661674347158218\n",
      "epoch 6 batch id 701 loss 2.224245548248291 train acc 0.8651925820256776\n",
      "epoch 6 batch id 751 loss 2.294083833694458 train acc 0.8662616511318242\n",
      "epoch 6 batch id 801 loss 0.02288537099957466 train acc 0.8624375780274657\n",
      "epoch 6 batch id 851 loss 0.1042778491973877 train acc 0.8636163337250293\n",
      "epoch 6 batch id 901 loss 1.5440468788146973 train acc 0.8645255271920089\n",
      "epoch 6 batch id 951 loss 0.23085610568523407 train acc 0.8650762355415352\n",
      "epoch 6 batch id 1001 loss 0.33122408390045166 train acc 0.8670079920079921\n",
      "epoch 6 batch id 1051 loss 1.0059008598327637 train acc 0.8677450047573739\n",
      "epoch 6 batch id 1101 loss 0.35475003719329834 train acc 0.8696071752951862\n",
      "epoch 6 batch id 1151 loss 0.42112600803375244 train acc 0.8717419635099913\n",
      "epoch 6 batch id 1201 loss 1.0683519840240479 train acc 0.8717214820982515\n",
      "epoch 6 batch id 1251 loss 0.8166858553886414 train acc 0.8735511590727418\n",
      "epoch 6 batch id 1301 loss 1.6289074420928955 train acc 0.8749519600307456\n",
      "epoch 6 batch id 1351 loss 0.23690348863601685 train acc 0.8760177646188009\n",
      "epoch 6 batch id 1401 loss 0.5026145577430725 train acc 0.8767844396859387\n",
      "epoch 6 batch id 1451 loss 0.09962804615497589 train acc 0.8771536871123363\n",
      "epoch 6 batch id 1501 loss 2.4169464111328125 train acc 0.8776232511658895\n",
      "epoch 6 batch id 1551 loss 0.8580114841461182 train acc 0.8784252095422308\n",
      "epoch 6 batch id 1601 loss 0.8893861770629883 train acc 0.8798797626483448\n",
      "epoch 6 batch id 1651 loss 0.6563348174095154 train acc 0.8796941247728649\n",
      "epoch 6 batch id 1701 loss 0.9360224008560181 train acc 0.8806584362139918\n",
      "epoch 6 batch id 1751 loss 1.7212438583374023 train acc 0.8812464306110794\n",
      "epoch 6 batch id 1801 loss 0.8015053272247314 train acc 0.8817670738478623\n",
      "epoch 6 batch id 1851 loss 1.7189204692840576 train acc 0.8821582928146947\n",
      "epoch 6 batch id 1901 loss 2.388132333755493 train acc 0.883022093634929\n",
      "epoch 6 batch id 1951 loss 0.012338504195213318 train acc 0.8840338288057407\n",
      "epoch 6 batch id 2001 loss 0.05134660378098488 train acc 0.8837456271864068\n",
      "epoch 6 batch id 2051 loss 0.5998611450195312 train acc 0.8833191126279863\n",
      "epoch 6 batch id 2101 loss 1.0777933597564697 train acc 0.8829426463588768\n",
      "epoch 6 batch id 2151 loss 0.03144645690917969 train acc 0.883687819618782\n",
      "epoch 6 batch id 2201 loss 0.2045442909002304 train acc 0.8836040436165379\n",
      "epoch 6 batch id 2251 loss 0.8398905992507935 train acc 0.883523989338072\n",
      "epoch 6 batch id 2301 loss 0.034761279821395874 train acc 0.8839091699261191\n",
      "epoch 6 batch id 2351 loss 0.02804645709693432 train acc 0.8843577201190983\n",
      "epoch 6 batch id 2401 loss 0.025974661111831665 train acc 0.8848136193252811\n",
      "epoch 6 batch id 2451 loss 1.0321643352508545 train acc 0.8847154222766218\n",
      "epoch 6 batch id 2501 loss 0.09063860774040222 train acc 0.8840213914434226\n",
      "epoch 6 batch id 2551 loss 0.2420218288898468 train acc 0.8835750686005488\n",
      "epoch 6 batch id 2601 loss 0.0557393804192543 train acc 0.8832900807381776\n",
      "epoch 6 batch id 2651 loss 0.2430247813463211 train acc 0.8842889475669559\n",
      "epoch 6 batch id 2701 loss 0.8366574048995972 train acc 0.884417808219178\n",
      "epoch 6 batch id 2751 loss 0.16747701168060303 train acc 0.8843375136314068\n",
      "epoch 6 batch id 2801 loss 0.16488423943519592 train acc 0.8848848625490896\n",
      "\n",
      "epoch 6 train acc 0.8850204408105226\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2cca5dbc32c40ffa5a6bb214ccf4e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 batch id 1 loss 0.893836259841919 train acc 0.875\n",
      "epoch 7 batch id 51 loss 0.06119949370622635 train acc 0.8970588235294118\n",
      "epoch 7 batch id 101 loss 0.8392494916915894 train acc 0.8886138613861386\n",
      "epoch 7 batch id 151 loss 0.17795896530151367 train acc 0.8853476821192053\n",
      "epoch 7 batch id 201 loss 1.043816328048706 train acc 0.8843283582089553\n",
      "epoch 7 batch id 251 loss 0.05219545587897301 train acc 0.8837151394422311\n",
      "epoch 7 batch id 301 loss 1.32590913772583 train acc 0.8824750830564784\n",
      "epoch 7 batch id 351 loss 0.4039571285247803 train acc 0.8883547008547008\n",
      "epoch 7 batch id 401 loss 0.0065556475892663 train acc 0.8855985037406484\n",
      "epoch 7 batch id 451 loss 1.0869872570037842 train acc 0.8899667405764967\n",
      "epoch 7 batch id 501 loss 0.03300292044878006 train acc 0.8919660678642715\n",
      "epoch 7 batch id 551 loss 1.8676316738128662 train acc 0.89258166969147\n",
      "epoch 7 batch id 601 loss 0.2760053277015686 train acc 0.8861272878535774\n",
      "epoch 7 batch id 651 loss 2.453744411468506 train acc 0.8876728110599078\n",
      "epoch 7 batch id 701 loss 2.0636143684387207 train acc 0.8859664764621968\n",
      "epoch 7 batch id 751 loss 2.0397300720214844 train acc 0.8875665778961385\n",
      "epoch 7 batch id 801 loss 0.5812667608261108 train acc 0.8848314606741573\n",
      "epoch 7 batch id 851 loss 0.028008822351694107 train acc 0.88689776733255\n",
      "epoch 7 batch id 901 loss 1.1310923099517822 train acc 0.888457269700333\n",
      "epoch 7 batch id 951 loss 0.11023952066898346 train acc 0.8893270241850684\n",
      "epoch 7 batch id 1001 loss 0.20357447862625122 train acc 0.8913586413586414\n",
      "epoch 7 batch id 1051 loss 0.03486381098628044 train acc 0.8918292102759277\n",
      "epoch 7 batch id 1101 loss 0.06805962324142456 train acc 0.8943006357856494\n",
      "epoch 7 batch id 1151 loss 0.19236540794372559 train acc 0.8959600347523893\n",
      "epoch 7 batch id 1201 loss 0.7893815040588379 train acc 0.8950874271440467\n",
      "epoch 7 batch id 1251 loss 0.5936228632926941 train acc 0.8968325339728217\n",
      "epoch 7 batch id 1301 loss 2.206338405609131 train acc 0.8982993850883936\n",
      "epoch 7 batch id 1351 loss 0.16252680122852325 train acc 0.8994726128793487\n",
      "epoch 7 batch id 1401 loss 0.08696020394563675 train acc 0.8997144896502498\n",
      "epoch 7 batch id 1451 loss 0.03414284437894821 train acc 0.9010165403170227\n",
      "epoch 7 batch id 1501 loss 1.722865104675293 train acc 0.9013574283810792\n",
      "epoch 7 batch id 1551 loss 0.09850732982158661 train acc 0.9025225660863959\n",
      "epoch 7 batch id 1601 loss 0.4525338411331177 train acc 0.9033806995627732\n",
      "epoch 7 batch id 1651 loss 0.899033784866333 train acc 0.9028619018776499\n",
      "epoch 7 batch id 1701 loss 0.8171355724334717 train acc 0.9038433274544385\n",
      "epoch 7 batch id 1751 loss 0.9771571755409241 train acc 0.9039834380354084\n",
      "epoch 7 batch id 1801 loss 0.5666732788085938 train acc 0.9044975013881177\n",
      "epoch 7 batch id 1851 loss 0.8190628290176392 train acc 0.9054902755267423\n",
      "epoch 7 batch id 1901 loss 1.9905163049697876 train acc 0.9060362966859548\n",
      "epoch 7 batch id 1951 loss 0.012359551154077053 train acc 0.906938749359303\n",
      "epoch 7 batch id 2001 loss 0.06384754925966263 train acc 0.9065779610194903\n",
      "epoch 7 batch id 2051 loss 0.004543447867035866 train acc 0.9064480741101901\n",
      "epoch 7 batch id 2101 loss 0.4436260461807251 train acc 0.9062351261304141\n",
      "epoch 7 batch id 2151 loss 0.01581166312098503 train acc 0.907107159460716\n",
      "epoch 7 batch id 2201 loss 0.05315496027469635 train acc 0.9074568378009995\n",
      "epoch 7 batch id 2251 loss 1.9244248867034912 train acc 0.9071246112838738\n",
      "epoch 7 batch id 2301 loss 0.033322397619485855 train acc 0.907132768361582\n",
      "epoch 7 batch id 2351 loss 0.022824421525001526 train acc 0.9074595916631221\n",
      "epoch 7 batch id 2401 loss 0.06893908977508545 train acc 0.9076686797167847\n",
      "epoch 7 batch id 2451 loss 0.6209402084350586 train acc 0.9076397388820889\n",
      "epoch 7 batch id 2501 loss 0.072173111140728 train acc 0.9072121151539384\n",
      "epoch 7 batch id 2551 loss 0.08532299101352692 train acc 0.9069727557820463\n",
      "epoch 7 batch id 2601 loss 0.012269914150238037 train acc 0.9064542483660131\n",
      "epoch 7 batch id 2651 loss 0.15004092454910278 train acc 0.9068511882308563\n",
      "epoch 7 batch id 2701 loss 0.5860897302627563 train acc 0.9068169196593854\n",
      "epoch 7 batch id 2751 loss 0.07702777534723282 train acc 0.9068520537986187\n",
      "epoch 7 batch id 2801 loss 0.08696459233760834 train acc 0.9074660835415923\n",
      "\n",
      "epoch 7 train acc 0.9077941699253466\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43e1f276dd194388991be9ee10d32d31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 batch id 1 loss 0.08752886950969696 train acc 1.0\n",
      "epoch 8 batch id 51 loss 0.02913292869925499 train acc 0.9154411764705882\n",
      "epoch 8 batch id 101 loss 0.4835392236709595 train acc 0.9071782178217822\n",
      "epoch 8 batch id 151 loss 0.1325603425502777 train acc 0.9060430463576159\n",
      "epoch 8 batch id 201 loss 0.5812167525291443 train acc 0.9076492537313433\n",
      "epoch 8 batch id 251 loss 0.0539797805249691 train acc 0.897410358565737\n",
      "epoch 8 batch id 301 loss 1.185375690460205 train acc 0.8978405315614618\n",
      "epoch 8 batch id 351 loss 0.26715177297592163 train acc 0.9038461538461539\n",
      "epoch 8 batch id 401 loss 0.00900849886238575 train acc 0.9036783042394015\n",
      "epoch 8 batch id 451 loss 0.4436936378479004 train acc 0.9067350332594235\n",
      "epoch 8 batch id 501 loss 0.03314460813999176 train acc 0.908807385229541\n",
      "epoch 8 batch id 551 loss 0.10402217507362366 train acc 0.9095961887477314\n",
      "epoch 8 batch id 601 loss 0.2489045262336731 train acc 0.9076539101497504\n",
      "epoch 8 batch id 651 loss 2.6365795135498047 train acc 0.9100422427035331\n",
      "epoch 8 batch id 701 loss 1.6564843654632568 train acc 0.9084343794579173\n",
      "epoch 8 batch id 751 loss 1.8236230611801147 train acc 0.9100366178428761\n",
      "epoch 8 batch id 801 loss 0.005654484499245882 train acc 0.9085518102372035\n",
      "epoch 8 batch id 851 loss 0.3761366009712219 train acc 0.9097385428907168\n",
      "epoch 8 batch id 901 loss 0.6658356189727783 train acc 0.9114178690344062\n",
      "epoch 8 batch id 951 loss 0.028641197830438614 train acc 0.9120005257623555\n",
      "epoch 8 batch id 1001 loss 0.4155190587043762 train acc 0.9128996003996004\n",
      "epoch 8 batch id 1051 loss 0.025154240429401398 train acc 0.9128211227402474\n",
      "epoch 8 batch id 1101 loss 0.6185513734817505 train acc 0.9149636693914623\n",
      "epoch 8 batch id 1151 loss 0.08519329130649567 train acc 0.9165942658557776\n",
      "epoch 8 batch id 1201 loss 0.5347046852111816 train acc 0.9153830141548709\n",
      "epoch 8 batch id 1251 loss 0.4194945693016052 train acc 0.9164168665067945\n",
      "epoch 8 batch id 1301 loss 1.849578857421875 train acc 0.9172751729438893\n",
      "epoch 8 batch id 1351 loss 0.08809886872768402 train acc 0.9181162102146558\n",
      "epoch 8 batch id 1401 loss 0.33213597536087036 train acc 0.9180942184154176\n",
      "epoch 8 batch id 1451 loss 0.01647326350212097 train acc 0.9188490696071675\n",
      "epoch 8 batch id 1501 loss 1.0050525665283203 train acc 0.9193454363757495\n",
      "epoch 8 batch id 1551 loss 0.040424883365631104 train acc 0.9200112830431979\n",
      "epoch 8 batch id 1601 loss 0.40979892015457153 train acc 0.9202841973766396\n",
      "epoch 8 batch id 1651 loss 0.9160492420196533 train acc 0.9189884918231375\n",
      "epoch 8 batch id 1701 loss 0.5395333766937256 train acc 0.9199368018812464\n",
      "epoch 8 batch id 1751 loss 0.6986185312271118 train acc 0.9205097087378641\n",
      "epoch 8 batch id 1801 loss 0.437824547290802 train acc 0.9206690727373681\n",
      "epoch 8 batch id 1851 loss 0.7213426828384399 train acc 0.9213938411669368\n",
      "epoch 8 batch id 1901 loss 1.602168321609497 train acc 0.9217517096265123\n",
      "epoch 8 batch id 1951 loss 0.006792120635509491 train acc 0.922924141465915\n",
      "epoch 8 batch id 2001 loss 0.44091129302978516 train acc 0.9225074962518741\n",
      "epoch 8 batch id 2051 loss 0.009275896474719048 train acc 0.9226901511457826\n",
      "epoch 8 batch id 2101 loss 0.050023265182971954 train acc 0.9224773917182294\n",
      "epoch 8 batch id 2151 loss 0.010772643610835075 train acc 0.9231171548117155\n",
      "epoch 8 batch id 2201 loss 1.4773099422454834 train acc 0.9232735120399819\n",
      "epoch 8 batch id 2251 loss 1.8256616592407227 train acc 0.9231452687694358\n",
      "epoch 8 batch id 2301 loss 0.02205677703022957 train acc 0.9229139504563233\n",
      "epoch 8 batch id 2351 loss 0.006035603582859039 train acc 0.9230380689068481\n",
      "epoch 8 batch id 2401 loss 0.0045203338377177715 train acc 0.9233652644731362\n",
      "epoch 8 batch id 2451 loss 0.536553144454956 train acc 0.9233986128110975\n",
      "epoch 8 batch id 2501 loss 0.09725356101989746 train acc 0.9232556977209116\n",
      "epoch 8 batch id 2551 loss 0.07160434871912003 train acc 0.922750882007056\n",
      "epoch 8 batch id 2601 loss 0.005843698047101498 train acc 0.9225778546712803\n",
      "epoch 8 batch id 2651 loss 0.5660959482192993 train acc 0.9232600905318747\n",
      "epoch 8 batch id 2701 loss 0.5438623428344727 train acc 0.9234774157719363\n",
      "epoch 8 batch id 2751 loss 0.0472579188644886 train acc 0.9238685932388222\n",
      "epoch 8 batch id 2801 loss 0.03757215291261673 train acc 0.9244019992859693\n",
      "\n",
      "epoch 8 train acc 0.92465783860647\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ac097c457f242b3b8e94658964ce751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 batch id 1 loss 0.06647329032421112 train acc 1.0\n",
      "epoch 9 batch id 51 loss 0.013598231598734856 train acc 0.9142156862745098\n",
      "epoch 9 batch id 101 loss 0.3521116375923157 train acc 0.9195544554455446\n",
      "epoch 9 batch id 151 loss 0.0018363241106271744 train acc 0.9221854304635762\n",
      "epoch 9 batch id 201 loss 0.5658109188079834 train acc 0.9228855721393034\n",
      "epoch 9 batch id 251 loss 0.05831364914774895 train acc 0.9220617529880478\n",
      "epoch 9 batch id 301 loss 0.6558061838150024 train acc 0.9233803986710963\n",
      "epoch 9 batch id 351 loss 0.2306309938430786 train acc 0.926994301994302\n",
      "epoch 9 batch id 401 loss 0.004605754278600216 train acc 0.9265897755610972\n",
      "epoch 9 batch id 451 loss 0.3317747712135315 train acc 0.928630820399113\n",
      "epoch 9 batch id 501 loss 0.016798539087176323 train acc 0.9286427145708582\n",
      "epoch 9 batch id 551 loss 0.05090568959712982 train acc 0.9284255898366606\n",
      "epoch 9 batch id 601 loss 0.3126387894153595 train acc 0.9255407653910149\n",
      "epoch 9 batch id 651 loss 2.123741388320923 train acc 0.9272273425499232\n",
      "epoch 9 batch id 701 loss 1.6533753871917725 train acc 0.9258202567760342\n",
      "epoch 9 batch id 751 loss 1.7152960300445557 train acc 0.9272636484687083\n",
      "epoch 9 batch id 801 loss 0.0011864788830280304 train acc 0.9261860174781523\n",
      "epoch 9 batch id 851 loss 0.007853249087929726 train acc 0.9269242068155111\n",
      "epoch 9 batch id 901 loss 0.4749707877635956 train acc 0.9284822419533851\n",
      "epoch 9 batch id 951 loss 0.02701924368739128 train acc 0.9293506834910621\n",
      "epoch 9 batch id 1001 loss 0.10344603657722473 train acc 0.9296953046953047\n",
      "epoch 9 batch id 1051 loss 0.021946756169199944 train acc 0.9300071360608944\n",
      "epoch 9 batch id 1101 loss 0.03400355204939842 train acc 0.9317665758401453\n",
      "epoch 9 batch id 1151 loss 0.06274649500846863 train acc 0.9329930495221547\n",
      "epoch 9 batch id 1201 loss 0.17732122540473938 train acc 0.9324521232306411\n",
      "epoch 9 batch id 1251 loss 0.33113765716552734 train acc 0.9327038369304557\n",
      "epoch 9 batch id 1301 loss 1.158179759979248 train acc 0.9337528823981552\n",
      "epoch 9 batch id 1351 loss 0.043209828436374664 train acc 0.9337527757216877\n",
      "epoch 9 batch id 1401 loss 0.34482160210609436 train acc 0.9331727337615988\n",
      "epoch 9 batch id 1451 loss 0.03440690040588379 train acc 0.9334941419710544\n",
      "epoch 9 batch id 1501 loss 1.0363805294036865 train acc 0.933960692871419\n",
      "epoch 9 batch id 1551 loss 0.28099626302719116 train acc 0.9343568665377177\n",
      "epoch 9 batch id 1601 loss 0.3432658016681671 train acc 0.9349625234228607\n",
      "epoch 9 batch id 1651 loss 0.7696042656898499 train acc 0.9340551181102362\n",
      "epoch 9 batch id 1701 loss 0.9802260398864746 train acc 0.9346707818930041\n",
      "epoch 9 batch id 1751 loss 0.11544996500015259 train acc 0.9346801827527127\n",
      "epoch 9 batch id 1801 loss 0.30929601192474365 train acc 0.9349666851749028\n",
      "epoch 9 batch id 1851 loss 0.7124727964401245 train acc 0.9355415991356024\n",
      "epoch 9 batch id 1901 loss 1.367510437965393 train acc 0.9358232509205682\n",
      "epoch 9 batch id 1951 loss 0.028855327516794205 train acc 0.9366350589441312\n",
      "epoch 9 batch id 2001 loss 0.0037070605903863907 train acc 0.9362193903048476\n",
      "epoch 9 batch id 2051 loss 0.00980006530880928 train acc 0.9364943929790346\n",
      "epoch 9 batch id 2101 loss 0.03804124891757965 train acc 0.9366075678248453\n",
      "epoch 9 batch id 2151 loss 0.0086152832955122 train acc 0.9370932124593212\n",
      "epoch 9 batch id 2201 loss 0.006079826503992081 train acc 0.9374148114493412\n",
      "epoch 9 batch id 2251 loss 1.2889466285705566 train acc 0.9372778764993336\n",
      "epoch 9 batch id 2301 loss 0.05814410373568535 train acc 0.9371468926553672\n",
      "epoch 9 batch id 2351 loss 0.0049910834059119225 train acc 0.9373936622713739\n",
      "epoch 9 batch id 2401 loss 0.0020158416591584682 train acc 0.9375260308204915\n",
      "epoch 9 batch id 2451 loss 0.3685312867164612 train acc 0.9376019991840066\n",
      "epoch 9 batch id 2501 loss 0.04740489274263382 train acc 0.9373000799680128\n",
      "epoch 9 batch id 2551 loss 0.0400019995868206 train acc 0.9368139945119561\n",
      "epoch 9 batch id 2601 loss 0.14198994636535645 train acc 0.9367791234140715\n",
      "epoch 9 batch id 2651 loss 0.038332171738147736 train acc 0.9375235760090532\n",
      "epoch 9 batch id 2701 loss 0.25624293088912964 train acc 0.9375925583117364\n",
      "epoch 9 batch id 2751 loss 0.03967737406492233 train acc 0.9378862231915667\n",
      "epoch 9 batch id 2801 loss 0.021578261628746986 train acc 0.9383925383791503\n",
      "\n",
      "epoch 9 train acc 0.9386553501599716\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9b5f9c767124d0ebe190919641c0ef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 batch id 1 loss 0.04791385680437088 train acc 1.0\n",
      "epoch 10 batch id 51 loss 0.025720154866576195 train acc 0.9301470588235294\n",
      "epoch 10 batch id 101 loss 0.13396503031253815 train acc 0.932549504950495\n",
      "epoch 10 batch id 151 loss 0.005933879874646664 train acc 0.9358443708609272\n",
      "epoch 10 batch id 201 loss 0.13361045718193054 train acc 0.9375\n",
      "epoch 10 batch id 251 loss 0.03144533187150955 train acc 0.9404880478087649\n",
      "epoch 10 batch id 301 loss 0.2751581072807312 train acc 0.9428986710963455\n",
      "epoch 10 batch id 351 loss 0.1719191074371338 train acc 0.9433760683760684\n",
      "epoch 10 batch id 401 loss 0.0032591994386166334 train acc 0.9435785536159601\n",
      "epoch 10 batch id 451 loss 0.2934042811393738 train acc 0.9438747228381374\n",
      "epoch 10 batch id 501 loss 0.00810663215816021 train acc 0.9434880239520959\n",
      "epoch 10 batch id 551 loss 0.08300015330314636 train acc 0.9422640653357531\n",
      "epoch 10 batch id 601 loss 0.17189796268939972 train acc 0.9395798668885191\n",
      "epoch 10 batch id 651 loss 1.4332994222640991 train acc 0.9417242703533026\n",
      "epoch 10 batch id 701 loss 1.2861738204956055 train acc 0.9403530670470756\n",
      "epoch 10 batch id 751 loss 1.5264148712158203 train acc 0.9417443408788282\n",
      "epoch 10 batch id 801 loss 0.000828807067591697 train acc 0.9406210986267166\n",
      "epoch 10 batch id 851 loss 0.009463028982281685 train acc 0.9411721504112809\n",
      "epoch 10 batch id 901 loss 0.18455557525157928 train acc 0.9419395116537181\n",
      "epoch 10 batch id 951 loss 0.013872461393475533 train acc 0.9430205047318612\n",
      "epoch 10 batch id 1001 loss 0.09217141568660736 train acc 0.942495004995005\n",
      "epoch 10 batch id 1051 loss 0.03524714708328247 train acc 0.9422573739295909\n",
      "epoch 10 batch id 1101 loss 0.1040164902806282 train acc 0.9432901907356949\n",
      "epoch 10 batch id 1151 loss 0.04207604378461838 train acc 0.9442875760208514\n",
      "epoch 10 batch id 1201 loss 0.17708373069763184 train acc 0.943796835970025\n",
      "epoch 10 batch id 1251 loss 0.23681685328483582 train acc 0.9441446842525979\n",
      "epoch 10 batch id 1301 loss 1.1553000211715698 train acc 0.9449942352036894\n",
      "epoch 10 batch id 1351 loss 0.031770169734954834 train acc 0.9449481865284974\n",
      "epoch 10 batch id 1401 loss 0.13256463408470154 train acc 0.9446823697359029\n",
      "epoch 10 batch id 1451 loss 0.007905184291303158 train acc 0.9450379048931771\n",
      "epoch 10 batch id 1501 loss 0.6067718267440796 train acc 0.9457028647568287\n",
      "epoch 10 batch id 1551 loss 0.06275457888841629 train acc 0.9458010960670535\n",
      "epoch 10 batch id 1601 loss 0.41909563541412354 train acc 0.9462835727670206\n",
      "epoch 10 batch id 1651 loss 0.9222663640975952 train acc 0.9450333131435493\n",
      "epoch 10 batch id 1701 loss 1.2583765983581543 train acc 0.9453630217519107\n",
      "epoch 10 batch id 1751 loss 0.12595504522323608 train acc 0.9451027984009137\n",
      "epoch 10 batch id 1801 loss 0.3109683692455292 train acc 0.9450999444752916\n",
      "epoch 10 batch id 1851 loss 0.7113983631134033 train acc 0.9453336034575905\n",
      "epoch 10 batch id 1901 loss 1.2310786247253418 train acc 0.9452261967385587\n",
      "epoch 10 batch id 1951 loss 0.45786696672439575 train acc 0.9459251665812404\n",
      "epoch 10 batch id 2001 loss 0.007636696100234985 train acc 0.9452461269365318\n",
      "epoch 10 batch id 2051 loss 0.00394274340942502 train acc 0.9451182350073135\n",
      "epoch 10 batch id 2101 loss 0.009250322356820107 train acc 0.9452046644455021\n",
      "epoch 10 batch id 2151 loss 0.006300557404756546 train acc 0.9457519758251975\n",
      "epoch 10 batch id 2201 loss 0.01083332672715187 train acc 0.9461892321671967\n",
      "epoch 10 batch id 2251 loss 2.4950714111328125 train acc 0.9458573967125722\n",
      "epoch 10 batch id 2301 loss 0.008826959878206253 train acc 0.9455128205128205\n",
      "epoch 10 batch id 2351 loss 0.003026047255843878 train acc 0.9458740961293067\n",
      "epoch 10 batch id 2401 loss 0.0015643145889043808 train acc 0.9460641399416909\n",
      "epoch 10 batch id 2451 loss 0.19361406564712524 train acc 0.9463229294165647\n",
      "epoch 10 batch id 2501 loss 0.08453947305679321 train acc 0.9461465413834467\n",
      "epoch 10 batch id 2551 loss 0.02656508795917034 train acc 0.9459525676205409\n",
      "epoch 10 batch id 2601 loss 0.0008285772055387497 train acc 0.9459582852748942\n",
      "epoch 10 batch id 2651 loss 0.06051136553287506 train acc 0.9466474915126367\n",
      "epoch 10 batch id 2701 loss 0.2561502456665039 train acc 0.9467789707515735\n",
      "epoch 10 batch id 2751 loss 0.014841245487332344 train acc 0.9471555797891675\n",
      "epoch 10 batch id 2801 loss 0.033956341445446014 train acc 0.947808818279186\n",
      "\n",
      "epoch 10 train acc 0.9479870245289727\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0.0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch_id, (input_ids_batch, attention_masks_batch, y_batch) in tqdm(enumerate(train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        y_batch = y_batch.to(device)\n",
    "        # 생각해보니까 내가 추가적으로 만든 embedding layer를 입력으로 주려면 모델 내부 구조를 바꿔야 되지 않나...?\n",
    "        # 우선 attention mask에 더해서 입력으로 줘보자\n",
    "        y_pred = model(input_ids_batch.to(device), attention_mask=attention_masks_batch.to(device))[0]\n",
    "        loss = cels_loss(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        train_acc += calc_accuracy(y_pred, y_batch)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(f\"epoch {epoch+1} batch id {batch_id+1} loss {loss.data.cpu().numpy()} train acc {train_acc / (batch_id+1)}\")\n",
    "\n",
    "    train_acc = train_acc / (batch_id+1)\n",
    "    print(f\"epoch {epoch+1} train acc {train_acc}\")\n",
    " \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    for batch_id, (input_ids_batch, attention_masks_batch, y_batch) in tqdm(enumerate(val_loader)):\n",
    "        y_batch = y_batch.to(device)\n",
    "        y_pred = model(input_ids_batch.to(device), attention_mask=attention_masks_batch.to(device))[0]\n",
    "        test_acc += calc_accuracy(y_pred, y_batch)\n",
    "        \n",
    "    print(f\"epoch {epoch+1} test acc {test_acc / (batch_id+1)}\")\n",
    "    \n",
    "    if test_acc >= best_acc:\n",
    "        best_acc = test_acc\n",
    "        torch.save(model.state_dict(), \"/opt/ml/models/0417_koelectra_eda.pt\")\n",
    "    \"\"\"\n",
    "    \n",
    "    if train_acc >= best_acc:\n",
    "        best_acc = train_acc\n",
    "        torch.save(model.state_dict(), \"/opt/ml/models/0417_koelectra_eda.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059b3cce-f28a-4be2-ac7e-3960de4bae41",
   "metadata": {},
   "source": [
    "### **6. 예측**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ac1c93d-b989-4afc-83fc-90a0c3779ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2f709f3854344898d2a18e6202fe018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"/opt/ml/models/0417_koelectra_eda.pt\"))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for input_ids_batch, attention_masks_batch, y_batch in tqdm(test_loader):\n",
    "    y_batch = y_batch.to(device)\n",
    "    y_pred = model(input_ids_batch.to(device), attention_mask=attention_masks_batch.to(device))[0]\n",
    "    _, predict = torch.max(y_pred, 1)\n",
    "    predictions.extend(predict.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21ade9c8-f187-49ff-bef0-bab6897dc6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(predictions, columns=['pred'])\n",
    "submission.to_csv(os.path.join(submission_dir, '0417_submission_1.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36edbc8-d63d-4037-9909-85befa78c694",
   "metadata": {},
   "source": [
    "### **7. 데이터 증강**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ada714-49b8-4b60-a35e-6d288c09860d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
