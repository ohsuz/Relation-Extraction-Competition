{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "516c357e-2e91-4c84-aafb-0747ff5db580",
   "metadata": {},
   "source": [
    "## [0416] KoElectra를 이용한 첫 베이스라인 가다듬기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ed7f8b-cdef-4dc7-9ec9-866c04786e2b",
   "metadata": {},
   "source": [
    "- **add_special_tokens 함수**: Entity 위치 정보를 활용해 [ENT],[/ENT] entity special token 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "553d190b-6eed-4c09-9d64-fc054d54e277",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mxnet in /opt/conda/lib/python3.7/site-packages (1.8.0.post0)\n",
      "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /opt/conda/lib/python3.7/site-packages (from mxnet) (1.18.5)\n",
      "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from mxnet) (0.8.4)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /opt/conda/lib/python3.7/site-packages (from mxnet) (2.23.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet) (2.9)\n",
      "Requirement already satisfied: gluonnlp in /opt/conda/lib/python3.7/site-packages (0.10.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (1.1.5)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (4.46.0)\n",
      "Requirement already satisfied: cython in /opt/conda/lib/python3.7/site-packages (from gluonnlp) (0.29.23)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from gluonnlp) (1.18.5)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from gluonnlp) (20.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->gluonnlp) (2.4.7)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.14.0)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (0.1.95)\n",
      "Requirement already satisfied: transformers==3 in /opt/conda/lib/python3.7/site-packages (3.0.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==3) (3.0.12)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from transformers==3) (1.18.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==3) (2021.4.4)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==3) (2.23.0)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from transformers==3) (0.1.95)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==3) (0.0.44)\n",
      "Requirement already satisfied: tokenizers==0.8.0-rc4 in /opt/conda/lib/python3.7/site-packages (from transformers==3) (0.8.0rc4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==3) (4.46.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers==3) (20.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3) (1.25.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==3) (2.9)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==3) (7.1.2)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==3) (1.0.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==3) (1.14.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers==3) (2.4.7)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (1.6.0)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch) (0.18.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch) (1.18.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install mxnet\n",
    "!pip install gluonnlp pandas tqdm\n",
    "!pip install sentencepiece\n",
    "!pip install transformers==3\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a61b8a2-2ff1-476c-bfa0-abd334cfb903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla V100-PCIE-32GB\n",
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla V100-PCIE-32GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tarfile\n",
    "import pickle as pickle\n",
    "from tqdm import tqdm\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Using KoELECTRA Model\n",
    "from transformers import ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\n",
    "\n",
    "# Added by Me\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from ohsuz.utils import *\n",
    "from ohsuz.loss import *\n",
    "from ohsuz.config import *\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59140c02-3c18-4028-bfdf-48de685580d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 128\n",
    "batch_size = 16\n",
    "warmup_ratio = 0.01\n",
    "epochs = 10\n",
    "max_grad_norm = 1\n",
    "log_interval = 50\n",
    "lr = 5e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3bb76b-256a-4827-8105-84ecf31ee504",
   "metadata": {},
   "source": [
    "### 1. Dataset & DataLoader 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5c9924-af61-4aa0-8986-a7c8695387af",
   "metadata": {},
   "source": [
    "**add_entity_tokens**\n",
    "- input\n",
    "    - entity token을 추가할 문장\n",
    "    - 첫 번째 entity 시작, 끝 index\n",
    "    - 두 번째 entity 시작, 끝 index\n",
    "- output\n",
    "    - 해당하는 index에 entity token이 추가된 문장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63286f78-b139-415d-bce6-bcbe2d487eb9",
   "metadata": {},
   "source": [
    "**make_embedding_layer**\n",
    "- input\n",
    "    - 문장의 input_ids\n",
    "- output\n",
    "    - entity에 해당하는 token이면 1, 아니면 0으로 나타내는 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "583f5ad7-635f-4f8c-b779-4e9f00a6a0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embedding_layer(input_ids, tokenizer):\n",
    "    flag = False\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    special_tokens = special_tokens_dict['additional_special_tokens']\n",
    "    is_entity_layer = []\n",
    "\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token in special_tokens:\n",
    "            if flag == False:\n",
    "                flag = True\n",
    "            else:\n",
    "                flag = False\n",
    "        else:\n",
    "            if flag == True:\n",
    "                is_entity_layer.append(1)\n",
    "                continue\n",
    "        is_entity_layer.append(0)\n",
    "\n",
    "    is_entity_layer = torch.tensor(is_entity_layer)\n",
    "    return is_entity_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05f6409f-b16f-403b-8512-d603da87f14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_entity_tokens(sentence, a1, a2, b1, b2):\n",
    "    new_sentence = None\n",
    "    special_tokens = special_tokens_dict['additional_special_tokens']\n",
    "    \n",
    "    if a1 > b1: # b1 먼저\n",
    "        new_sentence = sentence[:b1] + special_tokens[2] + sentence[b1:b2+1] + special_tokens[3] + sentence[b2+1:a1] + special_tokens[0] + sentence[a1:a2+1] + special_tokens[1] + sentence[a2+1:]\n",
    "        #new_sentence = sentence[:b1] + \"$\" + sentence[b1:b2+1] + \"$\" + sentence[b2+1:a1] + \"#\" + sentence[a1:a2+1] + \"#\" + sentence[a2+1:]\n",
    "    else: # a1 먼저\n",
    "        new_sentence = sentence[:a1] + special_tokens[0] + sentence[a1:a2+1] + special_tokens[1] + sentence[a2+1:b1] + special_tokens[2] + sentence[b1:b2+1] + special_tokens[3] + sentence[b2+1:]\n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01f23361-2dca-4d5b-8e1f-79149511f4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_dir, add_entity=True):\n",
    "    with open('/opt/ml/input/data/label_type.pkl', 'rb') as f:\n",
    "        label_type = pickle.load(f)\n",
    "    dataset = pd.read_csv(dataset_dir, delimiter='\\t', header=None)\n",
    "    dataset = preprocessing_dataset(dataset, label_type, add_entity)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def preprocessing_dataset(dataset, label_type, add_entity):\n",
    "    label = []\n",
    "    sentences = None\n",
    "    for i in dataset[8]:\n",
    "        if i == 'blind':\n",
    "            label.append(100)\n",
    "        else:\n",
    "            label.append(label_type[i])\n",
    "    \n",
    "    if add_entity:\n",
    "        ### 이 부분을 더 효율적으로 고치려면???\n",
    "        sentences = [add_entity_tokens(dataset[1][i], dataset[3][i], dataset[4][i], dataset[6][i], dataset[7][i]) for i in range(len(dataset))]\n",
    "    else:\n",
    "        sentences = dataset[1]\n",
    "\n",
    "    out_dataset = pd.DataFrame({'sentence':sentences,'entity_01':dataset[2],'entity_02':dataset[5],'label':label,})\n",
    "    return out_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f33bdd-9c7f-4b2f-b4cc-b8f39ca20d12",
   "metadata": {},
   "source": [
    "**handle_UNK의 option: REMOVE, ADD, REPLACE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "683a16c7-3d6a-4e39-aaf6-fdacdfdf0d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KoElecDataset(Dataset):\n",
    "    def __init__(self, tsv_file, add_entity=True, handle_UNK='REMOVE'):\n",
    "        self.dataset = load_data(tsv_file, add_entity)\n",
    "        self.dataset['sentence'] = self.dataset['entity_01'] + ' [SEP] ' + self.dataset['entity_02'] + ' [SEP] ' + self.dataset['sentence']\n",
    "        self.sentences = list(self.dataset['sentence'])\n",
    "        self.labels = list(self.dataset['label'])\n",
    "        self.tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "        self.tokenizer.add_special_tokens(special_tokens_dict)\n",
    "        self.handle_UNK = handle_UNK\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sentence, label = self.sentences[idx], self.labels[idx]\n",
    "        inputs = self.tokenizer(\n",
    "            sentence,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            max_length=190,\n",
    "            pad_to_max_length=True,\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "            \n",
    "        input_ids = inputs['input_ids'][0]\n",
    "        is_embedding_layer = make_embedding_layer(input_ids, self.tokenizer)\n",
    "        attention_mask = inputs['attention_mask'][0] + is_embedding_layer\n",
    "        \n",
    "        return input_ids, attention_mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27dfcf75-c013-4be0-9ab1-5a86644a5e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = KoElecDataset(os.path.join(train_dir, 'train.tsv'))\n",
    "test_dataset = KoElecDataset(os.path.join(test_dir, 'test.tsv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bad6ef-5713-47f2-84f5-fbd3a8c92265",
   "metadata": {},
   "source": [
    "**Train, Valid set 8 : 2 로 분리**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b3efc4f-3677-46a6-ab7d-d4b25ca74680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset, val_dataset = train_test_split(dataset, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "182770d7-d6b7-46a1-8925-d0f560ac78c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3cbc093-3346-4e1c-9b16-73e3170630b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=5)\n",
    "#val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=5)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, num_workers=5)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a6dca5f-c790-4edd-a677-45e5d37f2d4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[    2, 10086,  4239,  ...,     0,     0,     0],\n",
       "         [    2,  6802,     3,  ...,     0,     0,     0],\n",
       "         [    2,  6755,  7325,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [    2, 19729,  3210,  ...,     0,     0,     0],\n",
       "         [    2, 16156,    21,  ...,     0,     0,     0],\n",
       "         [    2,  2075,  4265,  ...,     0,     0,     0]]),\n",
       " tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " tensor([17,  0,  6,  2,  8,  0, 17,  3, 10,  0,  4,  0, 16,  4,  0,  0])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd31d4ea-ca3b-41a9-9ab0-2e337e15af30",
   "metadata": {},
   "source": [
    "### 2. Model 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0039622-dab3-49a8-ab92-da193b2ea4db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'electra.embeddings.position_ids']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = ElectraForSequenceClassification.from_pretrained(\"monologg/koelectra-base-v3-discriminator\", num_labels=42).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1b5680f-de99-4b49-81b2-93be56415040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(35004, 768)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(35004) # 내가 임의로 입력함 나중에 수정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a351c37e-f09c-4497-bf77-3dc2b6bcb093",
   "metadata": {},
   "source": [
    "### 3. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7efb6834-66c3-4477-903e-3b1399895718",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "ls_loss = LabelSmoothingLoss()\n",
    "cels_loss = CELSLoss()\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6) # 익효님꺼로 파라미터 변경\n",
    "# scheduler = CosineAnnealingLR(optimizer, T_max=2, eta_min=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63757f1d-9b2c-4d7b-9cb8-12dbdc47f109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d05d95b5-48df-420e-bba7-24f6c77d2982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6899395efe3d462e98c96e20e6478471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 7.421921730041504 train acc 0.0625\n",
      "epoch 1 batch id 51 loss 4.53863525390625 train acc 0.4693627450980392\n",
      "epoch 1 batch id 101 loss 4.202578544616699 train acc 0.48205445544554454\n",
      "epoch 1 batch id 151 loss 5.92665433883667 train acc 0.4830298013245033\n",
      "epoch 1 batch id 201 loss 4.1099958419799805 train acc 0.4779228855721393\n",
      "epoch 1 batch id 251 loss 3.4751670360565186 train acc 0.48705179282868527\n",
      "epoch 1 batch id 301 loss 3.8756747245788574 train acc 0.4925249169435216\n",
      "epoch 1 batch id 351 loss 2.4690780639648438 train acc 0.4976851851851852\n",
      "epoch 1 batch id 401 loss 2.436473846435547 train acc 0.506857855361596\n",
      "epoch 1 batch id 451 loss 2.8510470390319824 train acc 0.5192627494456763\n",
      "epoch 1 batch id 501 loss 3.9574692249298096 train acc 0.5259481037924152\n",
      "epoch 1 batch id 551 loss 2.321153163909912 train acc 0.5362976406533575\n",
      "\n",
      "epoch 1 train acc 0.5385213143872114\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e4ab756a3a4f43866db80e2817a4b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 3.862265110015869 train acc 0.5\n",
      "epoch 2 batch id 51 loss 3.675706624984741 train acc 0.6029411764705882\n",
      "epoch 2 batch id 101 loss 2.862703323364258 train acc 0.6175742574257426\n",
      "epoch 2 batch id 151 loss 3.450895071029663 train acc 0.6258278145695364\n",
      "epoch 2 batch id 201 loss 3.6509718894958496 train acc 0.6305970149253731\n",
      "epoch 2 batch id 251 loss 1.5159938335418701 train acc 0.6451693227091634\n",
      "epoch 2 batch id 301 loss 2.681750774383545 train acc 0.6536544850498339\n",
      "epoch 2 batch id 351 loss 1.4422131776809692 train acc 0.6582977207977208\n",
      "epoch 2 batch id 401 loss 1.1767903566360474 train acc 0.6658354114713217\n",
      "epoch 2 batch id 451 loss 2.254039764404297 train acc 0.6690687361419069\n",
      "epoch 2 batch id 501 loss 2.7259836196899414 train acc 0.6724051896207585\n",
      "epoch 2 batch id 551 loss 1.6676735877990723 train acc 0.6767241379310345\n",
      "\n",
      "epoch 2 train acc 0.6779529307282416\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca5272a25ea0448ebb893d897ad89354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 2.533492088317871 train acc 0.625\n",
      "epoch 3 batch id 51 loss 2.5197136402130127 train acc 0.6985294117647058\n",
      "epoch 3 batch id 101 loss 2.36226224899292 train acc 0.719059405940594\n",
      "epoch 3 batch id 151 loss 1.9987423419952393 train acc 0.7239238410596026\n",
      "epoch 3 batch id 201 loss 1.6767646074295044 train acc 0.7328980099502488\n",
      "epoch 3 batch id 251 loss 0.8858680725097656 train acc 0.7427788844621513\n",
      "epoch 3 batch id 301 loss 1.6513774394989014 train acc 0.7437707641196013\n",
      "epoch 3 batch id 351 loss 0.4920813739299774 train acc 0.7491096866096866\n",
      "epoch 3 batch id 401 loss 0.77606201171875 train acc 0.7563902743142145\n",
      "epoch 3 batch id 451 loss 1.4938093423843384 train acc 0.7595620842572062\n",
      "epoch 3 batch id 501 loss 3.432462692260742 train acc 0.7625998003992016\n",
      "epoch 3 batch id 551 loss 0.6788500547409058 train acc 0.7662205081669692\n",
      "\n",
      "epoch 3 train acc 0.7679840142095915\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11a07657954644efa27c8062a6bfa8c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1 loss 1.570448875427246 train acc 0.75\n",
      "epoch 4 batch id 51 loss 2.403407096862793 train acc 0.7720588235294118\n",
      "epoch 4 batch id 101 loss 1.1024820804595947 train acc 0.7951732673267327\n",
      "epoch 4 batch id 151 loss 1.3048317432403564 train acc 0.8054635761589404\n",
      "epoch 4 batch id 201 loss 1.5080339908599854 train acc 0.8106343283582089\n",
      "epoch 4 batch id 251 loss 0.6145176887512207 train acc 0.8187250996015937\n",
      "epoch 4 batch id 301 loss 1.8409743309020996 train acc 0.8108388704318937\n",
      "epoch 4 batch id 351 loss 0.5965124368667603 train acc 0.8167735042735043\n",
      "epoch 4 batch id 401 loss 0.6687719821929932 train acc 0.8206047381546134\n",
      "epoch 4 batch id 451 loss 1.1265949010849 train acc 0.8235864745011087\n",
      "epoch 4 batch id 501 loss 2.4620442390441895 train acc 0.8263473053892215\n",
      "epoch 4 batch id 551 loss 0.39743393659591675 train acc 0.8291742286751361\n",
      "\n",
      "epoch 4 train acc 0.830595026642984\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a774f36a6ea144faafeca9015327852d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 1 loss 1.0896188020706177 train acc 0.8125\n",
      "epoch 5 batch id 51 loss 1.7890093326568604 train acc 0.8419117647058824\n",
      "epoch 5 batch id 101 loss 0.8221771717071533 train acc 0.8508663366336634\n",
      "epoch 5 batch id 151 loss 0.9588762521743774 train acc 0.8497516556291391\n",
      "epoch 5 batch id 201 loss 1.6271013021469116 train acc 0.8526119402985075\n",
      "epoch 5 batch id 251 loss 0.6431394815444946 train acc 0.8578187250996016\n",
      "epoch 5 batch id 301 loss 1.026449203491211 train acc 0.8592192691029901\n",
      "epoch 5 batch id 351 loss 0.6144285798072815 train acc 0.8628917378917379\n",
      "epoch 5 batch id 401 loss 0.9620805978775024 train acc 0.864713216957606\n",
      "epoch 5 batch id 451 loss 1.6454386711120605 train acc 0.8621119733924612\n",
      "epoch 5 batch id 501 loss 2.1259446144104004 train acc 0.8614021956087824\n",
      "epoch 5 batch id 551 loss 0.34556692838668823 train acc 0.8637704174228675\n",
      "\n",
      "epoch 5 train acc 0.8644538188277087\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a1e6763b3e444e9cbb1edca091b0f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 batch id 1 loss 0.9451676607131958 train acc 0.875\n",
      "epoch 6 batch id 51 loss 0.8313095569610596 train acc 0.8774509803921569\n",
      "epoch 6 batch id 101 loss 0.6323969960212708 train acc 0.880569306930693\n",
      "epoch 6 batch id 151 loss 0.6782960891723633 train acc 0.882864238410596\n",
      "epoch 6 batch id 201 loss 0.6996987462043762 train acc 0.8883706467661692\n",
      "epoch 6 batch id 251 loss 0.9633588790893555 train acc 0.8919322709163346\n",
      "epoch 6 batch id 301 loss 0.9347851276397705 train acc 0.8920265780730897\n",
      "epoch 6 batch id 351 loss 0.1512310951948166 train acc 0.8945868945868946\n",
      "epoch 6 batch id 401 loss 0.5121674537658691 train acc 0.895573566084788\n",
      "epoch 6 batch id 451 loss 1.190389633178711 train acc 0.8957871396895787\n",
      "epoch 6 batch id 501 loss 1.6487905979156494 train acc 0.8958333333333334\n",
      "epoch 6 batch id 551 loss 0.3489508032798767 train acc 0.8975725952813067\n",
      "\n",
      "epoch 6 train acc 0.8983126110124334\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11e620894b6e4e3896c178baae6f3541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7 batch id 1 loss 0.7067004442214966 train acc 0.875\n",
      "epoch 7 batch id 51 loss 0.8358064889907837 train acc 0.9007352941176471\n",
      "epoch 7 batch id 101 loss 0.9635580778121948 train acc 0.8966584158415841\n",
      "epoch 7 batch id 151 loss 0.8095109462738037 train acc 0.8936258278145696\n",
      "epoch 7 batch id 201 loss 0.6405416131019592 train acc 0.898320895522388\n",
      "epoch 7 batch id 251 loss 0.6395898461341858 train acc 0.9001494023904383\n",
      "epoch 7 batch id 301 loss 0.5316004753112793 train acc 0.9013704318936877\n",
      "epoch 7 batch id 351 loss 0.3235449194908142 train acc 0.9050925925925926\n",
      "epoch 7 batch id 401 loss 0.3003685474395752 train acc 0.9067955112219451\n",
      "epoch 7 batch id 451 loss 0.6906238794326782 train acc 0.9070121951219512\n",
      "epoch 7 batch id 501 loss 1.1221625804901123 train acc 0.9089321357285429\n",
      "epoch 7 batch id 551 loss 0.13454334437847137 train acc 0.9090290381125227\n",
      "\n",
      "epoch 7 train acc 0.9096358792184724\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31f28e0d1b6e47689554a7ca741543b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8 batch id 1 loss 0.5552057027816772 train acc 0.875\n",
      "epoch 8 batch id 51 loss 0.3900566101074219 train acc 0.8970588235294118\n",
      "epoch 8 batch id 101 loss 0.3365430235862732 train acc 0.9077970297029703\n",
      "epoch 8 batch id 151 loss 1.7549057006835938 train acc 0.8981788079470199\n",
      "epoch 8 batch id 201 loss 1.102691650390625 train acc 0.8958333333333334\n",
      "epoch 8 batch id 251 loss 2.3128113746643066 train acc 0.8936752988047809\n",
      "epoch 8 batch id 301 loss 0.5011834502220154 train acc 0.8955564784053156\n",
      "epoch 8 batch id 351 loss 0.2679084241390228 train acc 0.8992165242165242\n",
      "epoch 8 batch id 401 loss 0.7614998817443848 train acc 0.90321072319202\n",
      "epoch 8 batch id 451 loss 0.5857945680618286 train acc 0.9068736141906873\n",
      "epoch 8 batch id 501 loss 0.9870500564575195 train acc 0.9089321357285429\n",
      "epoch 8 batch id 551 loss 0.07993300259113312 train acc 0.911978221415608\n",
      "\n",
      "epoch 8 train acc 0.9128552397868561\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50026508a15f48aea8b45c4eaf915591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9 batch id 1 loss 0.3934621810913086 train acc 1.0\n",
      "epoch 9 batch id 51 loss 0.6492621898651123 train acc 0.9350490196078431\n",
      "epoch 9 batch id 101 loss 0.1175699383020401 train acc 0.9405940594059405\n",
      "epoch 9 batch id 151 loss 0.44494080543518066 train acc 0.9412251655629139\n",
      "epoch 9 batch id 201 loss 0.5461031794548035 train acc 0.9406094527363185\n",
      "epoch 9 batch id 251 loss 0.22196003794670105 train acc 0.9422310756972112\n",
      "epoch 9 batch id 301 loss 0.3715178370475769 train acc 0.9424833887043189\n",
      "epoch 9 batch id 351 loss 0.08383341133594513 train acc 0.9423076923076923\n",
      "epoch 9 batch id 401 loss 0.5709728598594666 train acc 0.9431109725685786\n",
      "epoch 9 batch id 451 loss 0.6494795083999634 train acc 0.9440133037694013\n",
      "epoch 9 batch id 501 loss 0.7883056402206421 train acc 0.9451097804391217\n",
      "epoch 9 batch id 551 loss 0.11173073947429657 train acc 0.9452132486388385\n",
      "\n",
      "epoch 9 train acc 0.9456039076376554\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db4c6de76a3444798ef5bbdbebd7074f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 batch id 1 loss 0.984125554561615 train acc 0.9375\n",
      "epoch 10 batch id 51 loss 0.16507777571678162 train acc 0.9362745098039216\n",
      "epoch 10 batch id 101 loss 0.10816280543804169 train acc 0.9436881188118812\n",
      "epoch 10 batch id 151 loss 0.396756649017334 train acc 0.9466059602649006\n",
      "epoch 10 batch id 201 loss 0.7647391557693481 train acc 0.9458955223880597\n",
      "epoch 10 batch id 251 loss 0.159413680434227 train acc 0.9479581673306773\n",
      "epoch 10 batch id 301 loss 0.27500540018081665 train acc 0.9493355481727574\n",
      "epoch 10 batch id 351 loss 0.053770072758197784 train acc 0.9513888888888888\n",
      "epoch 10 batch id 401 loss 0.5284467339515686 train acc 0.9515274314214464\n",
      "epoch 10 batch id 451 loss 0.3354232609272003 train acc 0.9519124168514412\n",
      "epoch 10 batch id 501 loss 0.6443986892700195 train acc 0.9518463073852296\n",
      "\n",
      "epoch 10 train acc 0.9525976909413855\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0.0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch_id, (input_ids_batch, attention_masks_batch, y_batch) in tqdm(enumerate(train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        y_batch = y_batch.to(device)\n",
    "        # 생각해보니까 내가 추가적으로 만든 embedding layer를 입력으로 주려면 모델 내부 구조를 바꿔야 되지 않나...?\n",
    "        # 우선 attention mask에 더해서 입력으로 줘보자\n",
    "        y_pred = model(input_ids_batch.to(device), attention_mask=attention_masks_batch.to(device))[0]\n",
    "        loss = cels_loss(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        train_acc += calc_accuracy(y_pred, y_batch)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(f\"epoch {epoch+1} batch id {batch_id+1} loss {loss.data.cpu().numpy()} train acc {train_acc / (batch_id+1)}\")\n",
    "\n",
    "    train_acc = train_acc / (batch_id+1)\n",
    "    print(f\"epoch {epoch+1} train acc {train_acc}\")\n",
    " \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    for batch_id, (input_ids_batch, attention_masks_batch, y_batch) in tqdm(enumerate(val_loader)):\n",
    "        y_batch = y_batch.to(device)\n",
    "        y_pred = model(input_ids_batch.to(device), attention_mask=attention_masks_batch.to(device))[0]\n",
    "        test_acc += calc_accuracy(y_pred, y_batch)\n",
    "        \n",
    "    print(f\"epoch {epoch+1} test acc {test_acc / (batch_id+1)}\")\n",
    "    \n",
    "    if test_acc >= best_acc:\n",
    "        best_acc = test_acc\n",
    "        torch.save(model.state_dict(), \"/opt/ml/models/0416_koelectra_1.pt\")\n",
    "    \"\"\"\n",
    "    \n",
    "    if train_acc >= best_acc:\n",
    "        best_acc = train_acc\n",
    "        torch.save(model.state_dict(), \"/opt/ml/models/0416_koelectra_1.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059b3cce-f28a-4be2-ac7e-3960de4bae41",
   "metadata": {},
   "source": [
    "### **6. 예측**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ac1c93d-b989-4afc-83fc-90a0c3779ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3cf05a0135646728fcce948f3ad087a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=63.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"/opt/ml/models/0416_koelectra_1.pt\"))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for input_ids_batch, attention_masks_batch, y_batch in tqdm(test_loader):\n",
    "    y_batch = y_batch.to(device)\n",
    "    y_pred = model(input_ids_batch.to(device), attention_mask=attention_masks_batch.to(device))[0]\n",
    "    _, predict = torch.max(y_pred, 1)\n",
    "    predictions.extend(predict.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21ade9c8-f187-49ff-bef0-bab6897dc6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(predictions, columns=['pred'])\n",
    "submission.to_csv(os.path.join(submission_dir, '0416_submission_2.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b832495-1cb9-4a14-8ca6-8c1c9c2945ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
